![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.001.png)

See discussions, stats, and author profiles for this publication at:[ https://www.researchgate.net/publication/283341250](https://www.researchgate.net/publication/283341250_Geospatial_Big_Data_Handling_Theory_and_Methods_A_Review_and_Research_Challenges?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_2&_esc=publicationCoverPdf)

[Geospatial Big Data Handling Theory and Methods: A Review and Research Challenges](https://www.researchgate.net/publication/283341250_Geospatial_Big_Data_Handling_Theory_and_Methods_A_Review_and_Research_Challenges?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_3&_esc=publicationCoverPdf)

**Article** in ISPRS Journal of Photogrammetry and Remote Sensing · October 2015

DOI: 10.1016/j.isprsjprs.2015.10.012![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.002.png)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.003.png)

CITATIONS READS

491 7,580

**11 authors**, including:

[Songnian Li](https://www.researchgate.net/profile/Songnian-Li?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_5&_esc=publicationCoverPdf)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.004.png)

Toronto Metropolitan University (formerly Ryerson University) **164** PUBLICATIONS **3,845**CITATIONS

[SEE PROFILE](https://www.researchgate.net/profile/Songnian-Li?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_7&_esc=publicationCoverPdf)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.005.png)

[François Anton](https://www.researchgate.net/profile/Francois-Anton?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_5&_esc=publicationCoverPdf)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.006.png)

[Universidad Yachay Tech](https://www.researchgate.net/institution/Universidad-Yachay-Tech?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_6&_esc=publicationCoverPdf)

**209** PUBLICATIONS **1,440**CITATIONS

[SEE PROFILE](https://www.researchgate.net/profile/Francois-Anton?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_7&_esc=publicationCoverPdf)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.007.png)

[Suzana Dragicevic ](https://www.researchgate.net/profile/Suzana-Dragicevic?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_5&_esc=publicationCoverPdf)[Simon Fraser University](https://www.researchgate.net/institution/Simon-Fraser-University?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_6&_esc=publicationCoverPdf)

**145** PUBLICATIONS **5,031**CITATIONS![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.008.png)

[SEE PROFILE](https://www.researchgate.net/profile/Suzana-Dragicevic?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_7&_esc=publicationCoverPdf)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.009.png)

[Monika Sester](https://www.researchgate.net/profile/Monika-Sester?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_5&_esc=publicationCoverPdf)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.010.png)

[Leibniz Universität Hannover ](https://www.researchgate.net/institution/Leibniz-Universitaet-Hannover?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_6&_esc=publicationCoverPdf)**325** PUBLICATIONS **5,771**CITATIONS

[SEE PROFILE](https://www.researchgate.net/profile/Monika-Sester?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_7&_esc=publicationCoverPdf)![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.011.png)

All content following this page was uploaded by [Bin Jiang](https://www.researchgate.net/profile/Bin-Jiang-36?enrichId=rgreq-f468235d2c9311b22d1001a20c781ea0-XXX&enrichSource=Y292ZXJQYWdlOzI4MzM0MTI1MDtBUzo1NDkxOTE0NDY0NzA2NTZAMTUwNzk0ODg1MjMzMA%3D%3D&el=1_x_10&_esc=publicationCoverPdf) on 14 October 2017.

The user has requested enhancement of the downloaded file.

**\*Manuscript**

[**Click here to download Manuscript: PHOTO-D-15-00278 Revision.docx](http://ees.elsevier.com/photo/download.aspx?id=176747&guid=d4c96fd0-4c5e-4ab3-bd26-d18298990c31&scheme=1) **[Click here to view linked References**](http://ees.elsevier.com/photo/viewRCResults.aspx?pdf=1&docID=4587&rev=1&fileID=176747&msid={AFCB8900-7F3E-4155-84B7-D3E421024525})**

123    **Research Challenges** 

**Title: Geospatial Big Data Handling Theory and Methods: A Review and** 

` `4 

` `5 

` `6  **Authors (with equal contribution):** 

` `7 

` `8  Songnian Li \*, Ryerson University, Toronto, Canada,[ snli@ryerson.ca ](mailto:snli@ryerson.ca)

` `9 

10  Suzana Dragicevic, Simon Fraser University, Vancouver, Canada,[ suzanad@sfu.ca ](mailto:suzanad@sfu.ca)

11 

12  Francesc Antón Castro, Technical University of Denmark, Lyngby, Denmark,[ fa@space.dtu.dk ](mailto:fa@space.dtu.dk)

13 

14 

15  Monika Sester, Leibniz University Hannover, Germany,[ monika.sester@ikg.uni-hannover.de ](mailto:monika.sester@ikg.uni-hannover.de) 

16 

17  Stephan Winter, University of Melbourne, Australia,[ winter@unimelb.edu.au ](mailto:winter@unimelb.edu.au) 

18 

19  Arzu Coltekin, University of Zurich, Switzerland,[ arzu@geo.uzh.ch ](mailto:arzu@geo.uzh.ch) 

20 

21  Chris Pettit, University of New South Wales, Australia,[ c.pettit@unsw.edu.au ](mailto:c.pettit@unsw.edu.au)  

22 

23  Bin Jiang, University of Gävle, Sweden,[ bin.jiang@hig.se ](mailto:bin.jiang@hig.se) 

24 

25  James Haworth, University College London, UK,[ j.haworth@ucl.ac.uk ](mailto:j.haworth@ucl.ac.uk) 

26 

27  Alfred Stein, University of Twente, The Netherlands,[ stein@itc.nl ](mailto:stein@itc.nl) 

28 

29  Tao Cheng, University College London, UK,[ tao.cheng@ucl.ac.uk ](mailto:tao.cheng@ucl.ac.uk)

30 

31 

32  \* Corresponding author 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

66  66 60 

61 

62 

63 

64 

66 

123    **Review and Research Challenges** 

**Geospatial Big Data Handling Theory and Methods: A** 

` `4 

` `5 

` `6  Abstract:  Big  data  has  now  become  a  strong  focus  of  global  interest  that  is  increasingly 

` `7  attracting the attention of academia, industry, government and other organizations. Big data can 

` `8  be situated in the disciplinary area of traditional geospatial data handling theory and methods. 

` `9  The increasing volume and varying format of collected geospatial big data presents challenges 10  in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This 11  has implications for the quality of decisions made with big data. Consequently, this position 1123   paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical 

14  Commission II (TC II) revisits the existing geospatial data handling methods and theories to 15  determine if they are still capable of handling emerging geospatial big data. Further, the paper 16  synthesises  problems,  major  issues  and  challenges  with  current  developments  as  well  as 17  recommending what needs to be developed further in the near future.  

18 

19 

20  *Keywords: Big data, Geospatial, Data handling, Analytics, Spatial Modeling, Review* 

21 

22 

23 

24  **1.  Introduction** 

25 

26 

27  Over the last decade, big data has become a strong focus of global interest, increasingly attracting the attention 28  of academia, industry, government and other organizations. The term “big data” first appeared in the scientific 29  communities in the mid-1990s, gradually became popular around 2008 and started to be recognized in 2010. 30  Today, big data is a buzzword everywhere on the Internet, in the trade and scientific publications and during all 31  types of conferences. Big data has been suggested as a predominant source of innovation, competition and 32  productivity (Manyika et al. 2011), and has caused a paradigm shift to data-driven research (Kitchin 2014). The 33  rapid growing flood of big data, originating from the many different types of sensors, messaging systems and 34  social networks in addition to more traditional measurement and observation systems, have already invaded 35  many aspects of our everyday existence. On the one hand, big data, including geospatial big data, has great 36  potential to benefit many societal applications such as climate change, disease surveillance, disaster response, 37  monitoring critical infrastructures, transportation and so on. On the other hand, big data’s benefits to society are 38  usually limited by issues such as data privacy, confidentiality and security.  

39 

40  Big data is still not a clearly defined term and it has been defined differently from technological, industrial, 41  research or academic perspectives (Chen et al. 2014). In general, it is considered as structured and unstructured 42  datasets with massive data volumes that cannot be easily captured, stored, manipulated, analysed, managed and 43  presented by traditional hardware, software and database technologies. Along with its definitions, big data is 44  often described by its unique characteristics. In discussing application delivery strategies under increasing data 45  volumes, Laney (2001) first proposed three dimensions that characterise the challenges and opportunities of 46  increasing large data volumes: *Volume*, *Velocity* and *Variety* (3Vs). While *3Vs* have been continuously used to 47  describe big data, the additional dimension of *Veracity* has been added to describe data integrity and quality. 48  Further *Vs* have also been suggested such as variability, validity, volatility, visibility, value, and visualization. 49  However, these are met critically as they do not necessarily express qualities of magnitude. While it is true these 50  further *Vs* do not directly contribute to understanding the “big” in big data, they do touch on important concepts 51  related to the entire pipeline of big data collection, processing and presentation. Suthaharan (2014) even argued 52  that 3Vs cannot support early detection of big data  characteristics  for its classification and proposed 3Cs: 53  *cardinality*, *continuity*, and *complexity*. It is apparent that defining big data and its characteristics will be an 54  ongoing endeavour, but it nevertheless will not have negative impact on big data handling and processing.  

55 

56  According to the arguable phrase “80% of data is geographic” (see discussions in Morais (2012)), much of the 57  data  in  the  world  can  be  geo-referenced,  which  indicates  the  importance  of  geospatial  big  data  handling. 58  Geospatial data describe objects and things with relation to geographic space, often with location coordinates in 

66  66 60 

61 

62 

63 

64 

66 

` `1  a spatial referencing system. Geospatial data are usually collected using ground surveying, photogrammetry and  2  remote sensing, and more recently through laser scanning, mobile mapping, geo-located sensors, geo-tagged  3  web contents, volunteered geographic information (VGI), global navigation satellite system (GNSS) tracking  4  and so on. Adopting the widely accepted characterisation method, geospatial data can exhibit at least one of the  5  *3Vs* (Evans et al. 2014), but the other *Vs* mentioned above are also relevant. As such, geospatial big data can be  6  characterised by the following, with the first four being more fundamental and important: 

` `7 

` `8  §  Volume: Petabyte archives for remotely sensed imagery data, ever increasing volume of real-time sensor  9  observations and location-based social media data, vast amount of VGI data, etc., as well as continuous 10  increase of these data, raise not only data storage issues but also a massive analysis issue (Dasgupta 2013). 11  §  Variety: map data, imagery data, geotagged text data, structured and unstructured data, raster and vector 12  data, all these different types of data – many with complex structures – calls for more efficient models, 13  structures, indexes and data management strategies and technologies, e.g., use of NoSQL. 

14  §  Velocity:  imagery  data  with  frequent  revisits  at  high  resolution,  continuous  streaming  of  sensor 15  observations, Internet of Things (IoT), real-time GNSS trajectory and social media data all require matching 16  the speed of data generation and the speed of data processing to meet demand (Dasgupta 2013). 

17  §  Veracity: much of geospatial big data are from unverified sources with low or unknown accuracy, level of 18  accuracy varies depending on data sources, raising issues on quality assessment of source data and how to 19  “statistically” improve the quality of analysis results. 

20  §  Visualization: provides valuable procedures to impose human thinking into big data analysis. Visualizations 21  help analysts identifying patterns (such as outliers and clusters), leading to new hypotheses as well as 22  efficient ways to partition the data for further computational analysis. Visualizations also help end users to 23  better grasp and communicate dominant patterns and relationships that emerge from the big data analysis. 24  §  Visibility: the emergence of cloud computing and cloud storage has made it possible to now efficiently 25  access and process geospatial big data in ways that were not previously possible. Cloud technology is still 26  evolving and once issues such as data provenance – historical metadata – are resolved, big data and the 27  cloud would be mutually dependent and reinforcing technologies. 

28 

29  The increasing volume and varying format of collected geospatial big data pose additional challenges in storing, 30  managing, processing, analysing, visualizing and verifying the quality of data. Shekhar, et al. (2012, p. 1) states 31  that “the size, variety and update rate of datasets exceed the capacity of commonly used spatial computing and 32  spatial database technologies to learn, manage, and process the data with reasonable effort”. Big data tends to 33 

34  hcaopldacpiteoy opfle data to exanpalecyst ism(oGore manes d 2lar01g4er).  Verhypifoythinegses the thqat ualgitroywo ffgaeosterspatial than bthige dsata tatiansticald d ata strenprogdthu cts of ddelivata eranedd  35 

36  tothe enddelivuserereds isdnata otedproas duoctsne o(sfeethe 20b1ig2  cIhSallenPRS gResesoanldu tiobecn,om[www](http://www.isprs.org/documents/resolutions.aspx)es e[.](http://www.isprs.org/documents/resolutions.aspx)v[is](http://www.isprs.org/documents/resolutions.aspx)en[pr](http://www.isprs.org/documents/resolutions.aspx) [s](http://www.isprs.org/documents/resolutions.aspx)m[.o](http://www.isprs.org/documents/resolutions.aspx)o[r](http://www.isprs.org/documents/resolutions.aspx)r[g](http://www.isprs.org/documents/resolutions.aspx)e [/d](http://www.isprs.org/documents/resolutions.aspx)ch[ocu](http://www.isprs.org/documents/resolutions.aspx)allen[men](http://www.isprs.org/documents/resolutions.aspx)gi[ts](http://www.isprs.org/documents/resolutions.aspx)n[/r](http://www.isprs.org/documents/resolutions.aspx)g[es](http://www.isprs.org/documents/resolutions.aspx)in[olu](http://www.isprs.org/documents/resolutions.aspx)th[tio](http://www.isprs.org/documents/resolutions.aspx)e q[n](http://www.isprs.org/documents/resolutions.aspx)u[s](http://www.isprs.org/documents/resolutions.aspx)alit[.asp](http://www.isprs.org/documents/resolutions.aspx)y[x](http://www.isprs.org/documents/resolutions.aspx)co[)](http://www.isprs.org/documents/resolutions.aspx). nOntro l thoef 37  other hand, fitness of uses or purposes appears more valid or should be advocated (Mayer-Schönberger & 

38  Cukier 2013) in the context of big data. 

39 

40  The objectives of this paper are to (1) revisit the existing geospatial data handling methods and theories to 41 

42  determine if they are still capable of handling emerging geospatial big data; (2) examine current, state-of-the-art 43  methodological, theoretical, and technical developments in modelling, processing, analysing and visualising 44  geospatial big data; (3) synthesize problems, major issues and challenges in current developments; and  (4) 45  recommend what needs to be developed in the near future. Section 2 to 6 addresses objectives 1 and 2 of the 5 46  important areas related to geospatial big data handling methods and theories, which are the focus of various 47  Working Groups (WG) of ISPRS TC II. Related image analysis and processing topics, such as dimensionality 48  reduction; image compression; compressive sensing in big data analytics; content-based image retrieval; and 49  image endmember extraction, are not covered in this paper. Section 7 presents open issues and future research 50  directions of the three focus areas of TC II. Section 8 gives a summary and conclusions to the paper. 

51 

52 

53  **2.  Collection of Geospatial Big Data** 

54 

55  In recent years, along with the availability of new sensors, new ways of collecting geospatial data have emerged, 56  leading to completely new data sources and data types of geographical nature. Data acquired by the public, so- 57  called Volunteered Geographic Information (VGI), and data from geo-sensor networks have led to an increased 

58 

66  66 60 

61 

62 

63 

64 

66 

` `1  availability of spatial information. Whereas until recently, authoritative datasets were dominating in topographic  2  domain, these new data types extend and enrich geographic data in terms of thematic variation and by the fact  3  that it is more user-centric. The latter is especially true for VGI collected by social media (Sester et al. 2014).  

` `4 

` `5  Geospatial data collection is shifting from a data sparse to a data rich paradigm. Whereas some years back  6  geospatial data capture  was based on technically demanding,  accurate, expensive and complicated devices,  7  where the measurement process was itself sometimes an art, we are now facing a situation where geospatial data  8  acquisition is a commodity implemented in everyday devices such as smartphones used by many people. These  9  devices are capable of acquiring environmental geospatial information at an unprecedented level with respect to 10  greatly improved geometric accuracy, temporal resolution and thematic granularity. They are small, easy to 11  handle, and able to acquire data even unconsciously.  

12 

13  This data capture paradigm is similar to the situation in topographic data collection for digital terrain models by 14  capturing significant topographic points with morphological characteristics on the one hand (“qualified” points, 15  i.e.,  points  with  semantics)–  as  opposed  to  the  collection  of  point  clouds  using  LiDAR  sensors  or  stereo 16  matching, leading to masses of “unqualified” points (Ackermann 1994). The first approach requires manual 17  selection and measurement and guarantees that the topographic reality can be interpolated from the sparse 18  measurements.  The  second  approach  assumes  that  the  topographic  reality  is  captured  by  the  dense 19  measurements and can be reconstructed from them – thus the object formation and identification are shifted to 20  the analysis process.  

21 

22  In general, one can distinguish the following sensor configurations: (1) objects equipped with sensors moving 23  through space and capturing their own trajectories and the local environments: humans and moving devices such 24  as cars; and (2) static sensors constantly observing the (changing) environment. Today’s data acquired by these 25  new sensors and new stakeholders can be characterised as follows:  

26 

27  §  data streams, 

28  §  arbitrary high density, 

29  §  “close sensing” (Duckham 2013), i.e., the ability to measure many different dimensions of objects 30 

31  chdifarferaceternt isdticegrsee, e.sgo.,foppotical,sitio nacal ouacstic,cur acanyd  manecd hraenferical encefea, turarnesg,inang df rom highly precise coordinates via 32  §  relative positions to information where there is no geometric reference or is only implicitly located by 

33  location names. 

34 

35  There are many examples of data collections that may lead to geospatial big data sets. For example, from a 36  “social” perspective, over the last decade we have seen (through the rise of the so-called “smart city” concept) 37  the instrumentation of cities which are now providing vast amount of real-time data through the likes of smart 38  card ticketing systems, vehicle tracking devices, CCTV, toll systems, induction loops and other sensors. With 39  the rise of social media we are also seeing vast amounts of data (e.g., Twitter feeds), which can be geotagged 40  and used to assist in disaster management and emergency relief. From an “environmental science” perspective, 41 

42  there  are  huge  remotely  sensed  imagery  repositories  such  as  NASA’s  Landsat  repositories  which  provide 43  petabytes of geospatial data (Riebeek 2015). Capturing the urban environment is possible using a variety of 44  sensors. Cars (e.g., connected vehicles) are equipped with many sensors to aid the driver and enhance safety and 45  comfort. These sensors also capture the immediate environment of the car using  front cameras, backwards 46  cameras,  ultrasonic  (for  parking  assistance),  GPS,  radar,  rain-sensing  wipers  (Fitzner  et  al.  2013).  The 47  information is stored on local systems and can be transmitted to the available infrastructure or other vehicles.  

48 

49  Another important example concerns the quality of data on health. Such data are routinely collected and stored, 50  e.g., with doctors or health centres. In particular in public health centres, however, the coordinates in space and 51  time may lack quality. The first reason is that health aspects are not always related to the location where the 52  person lives. The second reason is that the moment when he or she is visiting the health facility may not 53  correspond  with  the  time  of  incidence.  As  an  important  reason  we  found  that  people  avoid  stigmatic 54  investigations, e.g., related to  AIDS or other sexually transmitted diseases, thus giving a bias  in routinely 55  collected datasets (Kandwal et al. 2010). 

56 

57  People, considered as “sensors”, can also help capture traffic or mobility related VGI style information. VGI 58  acquisition can be distinguished into participatory and opportunistic. Participatory data acquisition is conducted 

` `1  in  a  conscious  process  by  a  user,  who  explicitly  selects  objects  and  their  features  and  contributes  this  2  information (an example is the OpenStreetMap, OSM). Opportunistic data capture occurs unconsciously, mostly  3  with no specific purpose – or even a completely different purpose. A prominent example is the exploitation of  4  mobile phone data to determine traffic information such as traffic jams. The capture of the spatio-temporal  5  phenomenon “traffic jam” is just a by-product of many users having switched their phones on when driving their  6  cars. With the recent emergence of smart cards, transport ticketing systems like the London Oyster card are  7  capturing the movement of millions of travellers which use the London Tube and railway system daily. A new  8  data point is created every time they register the location via swiping on or off a mode of public transport. 

` `9 

10  VGI has proven to be an essential data source, when ad-hoc information is needed as in a disaster context  11  (Goodchild & Glennon 2010). In those situations, it is important to get any information – even if it is not very 12  accurate. Thus, social media and services are considered as new information sources  for example for early 13  response and crisis management (Fuchs et al. 2013). Fuchs et al. (2013) evaluated Twitter streams to detect 14  large scale flooding events in Germany. In a period of 8 months, approximately 6 million tweets had been 15  recorded. If the analysis concentrates only on the frequency, it was not possible to identify the events; however, 16  the inclusion of specific keywords, together with spatio-temporal clustering was able to detect some of the 17  events. A similar approach is reported by Dittrich and Lucas (2013). Huang et al. (2015) used millions of 18  location-based tweets to predict human movements. Other examples for the successful use of crowd-sourcing 19  data collection approach in the context of disasters, are the Haiti earthquake (http://www.ushahidi.com/), the 20  Queensland flood (McDougall 2011), as well as flood risk assessment (Poser et al. 2009). Invitation active 21  participation of users in the context of mapping has been reported by Frommberger et al. (2013). 

22 

23  It is worth noting that in the realm of geoscientific data, there is a wealth of new sensors and data sources, which 24  lead to large collections of diverse and “dirty” data (inaccurate, incomplete or erroneous data), which only gain 25  relevance by careful integration and fusion with complementary data (Van Zyl et al. 2009).  

26 

27 

28  **3.  Quality Assessment** 

29 

30 

31  Geospatial data, being abstractions and observations of a continuous reality (Frank 2001), is by nature uncertain, 32  ideally time-stamped and often incomplete. Hence, geospatial big data, with its defining characteristics of being 33  large (voluminous), heterogeneous (variety), real-time processed (velocity), inconsistent (variability), and thus 34  also of variable quality (veracity), must suffer even more from uncertainty, asynchronicity, and incompleteness. 35  However, while certain effects on data quality are emphasized for geospatial big data, the phenomena to be 36  described are still the same. Thus, the known methods and theories of quality assessment are still applicable. 

37 

38  In  geographic  information  science  and  technology,  standardized  methods  have  been  developed  in  order  to 39  assess, describe and propagate quality characteristics both quantitatively and qualitatively. Frameworks exist to 40  describe data quality from a producer's perspective, which then, in the hands of consumers, have to be translated 41  into  fitness  for purpose. These  frameworks  went into international standards, such as ISO 8402 (which is 42  generically about quality management: “Data quality is the totality of characteristics of a product that bear on its 43  ability to satisfy stated and implied needs”) or ISO 19157 (which is specifically about the quality of geographic 44  information: “Establishes the principles for describing the quality of geographic data. It defines components for 45  describing data quality, specifies components and content structure of a register for data quality measures, 46  describes  general  procedures  for  evaluating  the  quality  of  geographic  data,  and  establishes  principles  for 47  reporting data quality”). The frameworks typically define quantitative measures of data quality, such as spatial, 48  temporal  and  thematic  accuracy,  spatial,  temporal  and  thematic  resolution,  consistency,  and  completeness 49  (Veregin 2005), and in addition qualitative characteristics of data quality, such as purpose, usage, or lineage. 

50 

51  In the end, this approach to data quality assessment is descriptive about the data capturing process, stored in 52  separate metadata. But these data are used in decision making processes, and thus, the quantities and qualities of 53 

54  thdoe ne abboyvVe afnradme eVwolargk set reqal.u(ir2e0 0f5u)rtanherd  anVanalydse isVfolargparnodp agStatioein n(2. 0I0n6a ) osnp atial natursal tatiosbtical jects coanndtexKot hwlioet rk al.has (2b0ee12n) 55  on  slums.  Frank  (2007,  p.  417),  studying  the  ontology  of  imperfect  knowledge,  stated:  “How  do  the 

56  imperfections in the data affect the decision?”, but acknowledges that the decision making process is a black 57  box  of  unknown  complexity.  While  still  some  quality  descriptors,  especially  the  quantitative  ones,  lend 58 

` `1  themselves to functional error propagation, others – especially the qualitative ones – are still left to human  2  judgment. With the ad-hoc combination of data streams in geospatial big data collection and near-real-time  3  analytics, this traditional approach falls short both in collecting, aggregating or propagating metadata as well as  4  in human judgment of metadata. These challenges can be illustrated for example in the context of connected  5  urban transport introduced above: 

` `6 

` `7  §  The sheer volume of geospatial big data in transport arises from the large number of agents (vehicles,  8  people, and goods) on their way at any time. Furthermore, in order to be able to predict transport  9  demand or traffic, not only  are  real-time data  required but also historic data. If the bandwidth of 10  communication channels forms bottlenecks, even with lossless compression, either the sampling rate or 11  the sampling size can be reduced (loss of details), or the computation can be decentralized (Duckham 12  2013), in which case a central instance would collect only aggregated data. Each of these solutions has 13  an impact on the quality assessment of the collected data. 

14  §  The large variety of geospatial big data in transport (such as GNSS, inertial sensors, compass, wheel 15  sensors, radar, laser scanning, number plate recognition, induction loops, electronic toll or ticketing, 16  parking sensors, social media comments, citizen reports, to name only a few) to be combined in big 17  data analytics challenges error propagation models, which are based on a functional relationship. 

18  §  The velocity of data, more often than not requiring real-time analysis for event management or near- 19  future prediction, does not allow for setting up proper error propagation, since disturbances in the 20  sensor or channel can only be detected in hindsight or from prediction models. 

21  §  The variability of geospatial big data in transport, indicating inconsistency, stems from both the variety 22  between different data channels as well as the unreliability of any of these sources. For example, 23  volunteered geographic information can be inconsistent if the “citizens as sensors” (Goodchild 2007) or 24  “citizens as databases” (Richter & Winter 2011) disagree on their observations, are just not sampling a 25  particular phenomenon of interest (e.g., if a tree falls on a street and nobody is there to notice), or are 26  limited by lack of communication channels (e.g., a sensor is moving out of reach of WiFi/cell phone 27  coverage). In another example, satellite positioning has some well-known quality descriptors, but in 28  urban canyons, these measures vary with the location in the environment rather than the sensor, the 29  transmission channel or the time (Kealy et al. 2014). This dependency on location is non-linear and 30  difficult to predict. 

31  §  The veracity of geospatial big data in transport indicates already that data is to be combined of very 32  different quality. This extends also to irregular sampling rates (both spatial and temporal), entry errors, 33  redundancy,  corruption,  lack  of  synchronization,  or  a  variety  of  collection  purposes  (taxonomies, 34  semantics), to name a few. 

35  §  Variability and veracity are closely tied to vulnerability, and vulnerability is perhaps the only aspect 36  that is in contrast to classic institutional databases, which are behind firewalls or even completely 37  disconnected  (in  safety  critical  applications).  Since  both  big  data  collection  and  analytics  require 38  connectivity, concepts are also required to deal with malicious contributions, attacks and theft, and 39  privacy. The latter is particularly true for geospatial data collected for tracking movements, and a 40  measure of quality would cater for protection of privacy while still guaranteeing some level of quality 41 

42  of service (Anthony et al. 2007, Duckham & Kulik 2006). 

43 

44  Within  geospatial  big  data  in  transport,  a  prominent  example  is  research  that  observes  the  quality  of 45  OpenStreetMap, which has become an open source for navigation services at a global scale. This research 46  focuses mostly on the completeness of OpenStreetMap (Haklay 2010, Mondzech & Sester 2011, Neis et al. 47  2012, Zielstra & Zipf 2010). 

48 

49  Typically, the value of geospatial big data (analytics) is seen in information for decision support. Analytical 50  methods such as data mining and machine learning enable only inductive reasoning on big data, i.e., detection of 51  global correlations, or predictions based on these correlations. In transport, an example is the early discovery of 52  traffic accidents. In these applications the traditional quality (from a provider or consumer perspective) has been 53  replaced by correlation coefficients (Miller & Goodchild 2014), well knowing that correlation is not necessarily 54  about causes or truth. Thus, validity or trust is traded for the velocity of information production. 

55 

56 

57 

58 

66  66 60 

61 

62 

63 

64 

66 

` `1  **4.  Data Modeling and Structuring** 

` `2 

` `3 

` `4  All the spatial data models, including the spaghetti vector data model, the network data model, the topological  5  data model and the regular (raster) and irregular (Voronoi, k-d-tree, Binary Space Partitioning Tree, paving  6  group, crystallographic group) tessellations based data models, can be used for handling geospatial big data.  7  Nevertheless, there are some models that are more suitable for handling very large data sets and others that are  8  less suitable for geospatial big data. As the network and topological data models need to store the connectivity  9  (for the network and topological spatial data model) and the adjacency (for the topological spatial data model), 10  they are not well suited for handling geospatial big data streams unless a very efficient spatial data indexing is 11  making the update of the connectivity and/or the topology possible in real time. Since geospatial big data 12  environments might have to relax accuracy constraints to satisfy the real-time constraint, irregular and especially 13  regular tessellations are ideally suited to handle soft errors – geometric uncertainties in data that do not cause 14  erroneous  topology.  Finally,  as  regular  tessellations  can  be  stored  in  matrices,  they  are  subject  to  very 15  parallelized  vectorization  algorithms.  Nevertheless,  the  implicit  topology  of  the  regular  tessellation  (raster) 16  spatial data model might not be desired topology in some applications that require small inaccuracies (e.g., 17  collision detection for driver-less cars). In such cases, the only possible way to satisfy real time constraints is to 18  use a spatial data indexing method that can maintain its performances with a big data stream being updated in 19  real  time.  Current  spatial  data  indexing  methods  cannot  handle  geospatial  big  data  streams,  because  their 20  efficiency gets lower as new spatial data streams go over the capacity of extension of the spatial data index (e.g., 21  all the available locations for a hash function value get spent and the hash function needs to get updated or a tree 22  must be rebalanced after a series of additions of data from the stream).  

23 

24  Spatial statistics is  well suited to handle big data. It  offers capabilties to summarize the data, and express 25  measures of variation and uncertainty. The main concern, however, is that many of the processes and procedures 26  are developed for smaller datasets. In particular, much of spatial statistical analysis is either done on datasets 27  that are collected at a pointwise scale (such as field data, meteorological data, or administrative data) and of a 28  relatively small content, or is focusing on the relatively large image data sets which have a very specific nature. 29  Spatial statistics depends upon the notion of spatial (and spatio-temporal) dependence, and such dependence in 30  turn depends upon the notion of distance between points. For n observations, including their coordinates in 31  space or space and time, evaluating distances requires inspection of n2 pairs of points, and here steps should be 32  made to be able to do this efficiently. The current data structures as such are usually able to handle the big data 33  as  well,  but  most  likely  specific  procedures  have  to  be developed  that  are  able  to  address  issues  that  are 34  relatively novel (such as combining data in the space-time domain) or that have to address specific questions 35  and problems, i.e., to select data from a big data set for a particular model application. A particular way ahead 36  may be that classification of the data into multiple classes is done in the form of metadata. In such a way it is 37  possible to make the big data of relevance in a wide range of practical applications. This would require an 38  improved database structure, and in particular a very much adaptive spatial statistical analysis procedure. 

39 

40  Some authors have already pointed out the necessity of parallel and distributed programming for handling the 41  big data sets in the general context or even in the geospatial context (Lee et al. 2014, Shekhar et al. 2012, 42  Shekhar et al. 2014, Wang et al. 2013). Others have pointed out the usefulness of functional programming 43  concepts or languages such as Haskell Domain-Specific Language (Mintchev 2014), Map-reduce (Maitrey & 44  Jha 2015, Mohammed et al. 2014), Data Flow Graphs (Tran et al. 2012), or self-adjusting computation (Acar & 45  Chen  2013).  However,  there  is  a  gap  between  the  research  works  that  advocate  functional  programming 46  techniques but do not handle specifically geospatial data, and research works that focus on geospatial big data, 47  but do not guarantee the absence of data races which are the races of different threads to gain access to the same 48  data item in some shared memory Milewski (2009). The following sections discuss issues related to functional 49  programming  paradigms  for  big  data  streams  and  geospatial  big  data  analytics  in  the  context  of  big  data 50  modeling and sturcturing, and examines how geospatial data models and structures are adapted to big data. 

51 

52  ***4.1  Functional programming for big data streams*** 

53 

54 

55  The main stumbling block for handling geospatial big data streams using parallel programming and the best 56  reason for using functional programming is the concept of data races. A data race is a race between different 57  threads that try to access the same data items, and relates to the notion of concurrency. Functional programming 58  solves the problem of data races by strictly controlling the simultaneous access to mutable data. It has been 

66  66 60 

61 

62 

63 

64 

66 

` `1  predicted that data races will produce the “downfall of imperative programming” (Milewski 2009). The main  2  advantage of Haskell (or other functional programming languages like Closure, Lisp, ML, Scheme) for big data  3  is its support of parallel computing and concurrency and the high performance of the most famous Haskell  4  compiler  (GHC:  the  Glasgow  Haskell  Compiler):  “applications  built  with  GHC  enjoy  solid  multicore  5  performance and can handle hundreds of thousands of concurrent network connections” (Mintchev 2014).  

` `6 

` `7  Domain Specific Languages (DSL) provide a solution to the key challenge of big data by making the “multi-  8  disciplinary collaboration as  effective and productive as  possible”  and by offering the “required degree of  9  flexibility  and  control”  and  a  domain-specific  development  completed  on  time  possibly  by  non-software 10  developers (Mintchev 2014). The functional programming languages map and reduce functions are the basis of 11  the MapReduce programming model for processing big data sets by a distributed parallel algorithm. Maitrey & 12  Jha (2015) states “MapReduce has emerged as the most popular computing paradigm for parallel, batch-style 13  and analysis of large amount of data”, especially since Google adopted it.  

14 

15  ***4.2  Geospatial big data analytics*** 

16 

17 

18  We can classify the techniques used for spatial data mining from different points of view: the assumptions that 19  these techniques pre-suppose and the  “curse of dimensionality” that they exhibit or  not. While parametric 20  statistics assume some probability distribution function or some spatial distribution, non-parametric statistics 21  only  assume  local  smoothness.  Functional  analysis  (e.g.,  wavelets)  and  homotopy  continuation  techniques 22  assume only the continuity of the functions involved. The “curse of dimensionality” (Juditsky et al. 1995) states 23  that the number of points needed to train machine-learning algorithms gets exponential with the dimension of 24  the search space. Statistical and machine learning techniques and even statistics based dimensionality reduction 25  algorithms do not lower the dimensionality of the problem in a deterministically exact way (Li et al. 2011). 26  Thus, they exhibit the “curse of dimensionality” (Juditsky et al. 1995). On the contrary, homotopy continuation 27  techniques are not subject to an exponential growth of the number of samplings (because they do not rely on 28  training with points) and the uncertainty of the modeling does not explode from one dimension to the next one, 29  as this has been shown in Musiige et al. (2013) and Musiige et al. (2011). This observation can be extended to 30  other functional analysis techniques as wavelets (Juditsky et al. 1995). 

31 

32  As it can be conceived, in any attempt to process big spatial data streams in real time, one might be tempted to 33  ask if tolerating soft errors could be feasible, and to what extent. This was addressed in Carbin et al.  (2013). If 34  we do not accept soft errors, then we need to rely on new High Performance Computing architectures to harness 35  the parallelism necessary to process geospatial big data streams in real time (Carbin et al. 2013). However, in 36  order  to  analyze  and  compare  geospatial  big  data  algorithms,  we  need  benchmarks,  so  that  the  different 37  algorithms can have a common evaluation basis. Big data benchmarks (Shekhar et al. 2014, Shekhar et al. 2012) 38  have become one fundamental concept in studying geospatial big data.  

39 

40  Spatial databases research has intended to make query processing faster by designing spatial indexing methods, 41  which partition the search space in tiles (possibly irregular), so that the average query time will be concentrated 42  in one tile. The main challenge is to cleverly organize geospatial data in tiles through spatial data clustering that 43  will define optimal tiles and efficient paths (space-filling curves) through these tiles, so that the access to n- 44  dimensional data is done efficiently by referring to the location of the tile along  that “space-filling curve”. 45  Several space-filling curves have been proposed in the literature. However, the Hilbert space-filling curve has 46  the advantage that the arc length between two consecutive tiles along the space-filling curve is constant (Ujang 47  et al. 2014). The main use and value of geospatial big data is to dig useful information from geospatial big data 48  sets (Wang et al. 2013, Wang & Yuan 2013, 2014).  

49 

50 

51  **5.  Data Visualization and Visual Analytics** 

52 

53 

54  When several additional Vs are proposed in defining the big data (see Section 1 above), it is no surprise that the 55  terms *visualization* and *visual analytics* are frequently mentioned. There are many strong reasons for this with 56  the primary one being that some of our computational and statistical approaches do not scale – there is simply 57  *too much* data (Keim et al. 2013, Shneiderman 2014). Even smaller amounts of data in forms and tables are not 58  really human readable, thus the interactive and exploratory visualization environments help at the very early 

66  66 60 

61 

62 

63 

64 

66 

` `1  stages in dealing with big data in making sense of what the data actually contains (Cook et al. 2012, Frankel &  2  Reid 2008, Hoffer 2014). In other words, visualizations essentially enable humans to deal with big data where  3  machines along might fall short. Conversely, visual analytics approaches acknowledge the human shortcomings  4  as well and combine the powers of computational tools with powers of human visual sense-making (Choo &  5  Park 2013). Visualizations, therefore, are widely acknowledged as a part of analysis process (i.e., not only  6  communication), in which we can explore the data, and build hypotheses during this process (Zhang et al. 2012).  7  It is important to note that visualizations have been more commonly conceptualized as *communication* tools.  8  While this is true and visualizations are important in communicating hypotheses, results and ideas, in the case of  9  big data, we believe their role in *exploration* plays an even more important part. However, visualizations, while 10  often offered as remedies to the shortcomings of the computational methods, may not scale either. Big data 11  means  many  things  to  display;  and  it  often  results  in  very  ‘busy’  displays,  especially  given  the  trend  for 12  multiple-linked view displays that are popular in visual analytics and big data applications. Such requirements 13  can lead to information overload. It is important to note that human cognitive resources (such as the visual 14  working memory that is critical in processing visual information; or spatial abilities which are critical for how 15  well  we  can  make  sense  of  visualizations)  are  limited  (e.g.,  Hegarty  2011,  Hegarty  et  al.  2012).  Novel 16  visualization designs are necessary, especially those informed by knowledge on human information processing, 17  perception and cognition. 

18 

19  In terms of novel design and visualization paradigms, the field witnessed many alternative approaches that are 20  constantly being proposed, even though many of them are not yet validated through empirical testing. Among 21  these approaches, the most dominant one appears to be the *multiple-linked views*, in which approaches such as 22  brushing  and  linking  are  used  to  allow  the  viewer  to  work  with  various  visualizations  at  the  same  time 23  (Bernasocchi  et  al.  2012).  Alternatively,  summarization,  clustering  and  highlighting  approaches  have  been 24  proposed.  Vision-inspired  approaches,  such  as  *focus+context*  visualizations  and  *foveation*  may  provide 25  interesting new opportunities as they attempt to reduce the information load and are becoming more relevant 26  with  the  recent  technological  developments  offering  cheaper  and  better  eye  tracking  solutions  (Bektas  & 27  Çöltekin 2012, Cockburn et al. 2008, Çöltekin 2009). Other approaches have also been proposed in the literature 28  taking advantage of technological developments such as *cloud computing*, *parallel processing*, *indexing and* 29  *querying* for real time utilization (e.g., Chen et al. 2014, Hoffman 2012, Liu et al. 2013, Lu et al. 2013). 

30 

31  Despite an influx of “new” solutions, we have also witnessed a rediscovery of a “not so new” system -- through 32  a strong coupling between geographic information systems (GIS) and big data. GIS is an unmatched and mature 33  toolbox for *data science* as its abilities to process spatial and non-spatial (attribute) data (even when they are not 34 

35  pperarftsecotlfya strwuhctoule”red()Deothroguawgha ncokam2p0u1tatio4). nGeoal as grapwell hic as invfoisrumal atiomneanscies. Sonceme anedvenrelatecaldledd oGImaS inans ds ubcigh  as data rem“towte o 36  sensing and geoinformatics have been dealing with large datasets for a relatively long time, before the term big 

37 

38  dadata ventot ookf  mbigomdeatantu,mothiners cbieranncech, epsopouf largeocugrltuaprhe y anhdavbe uasinlsoesss h(oÇwönltegkrineat &inRtereichest eninb auchtilierzi2n0g11b)ig. Wdata ith thfoe r 39  addressing social (human geography) and environmental (physical geography) questions (e.g., Crampton et al. 

40  2013, Goodchild 2013, Kitchin 2013, Steed et al. 2013, Wood et al. 2013).  

41 

42 

43  One of the challenges is to be able to make such geospatial big data accessible to end users so that it can be used 44  to make real world decisions. Data visualization tools and techniques are therefore critical in providing windows 45  into such rich data so that it can be analyzed and interrogated by researchers, policy and decision-makers and 46  citizens alike.  World Wide Web platforms, such as *geoportals*, provide an excellent means to deliver such 47  services.  Portals  provide  access  to  geospatial  data,  and  there  has  been  significant  momentum  in  creating 48  federated  geoportal,  which  can  access  a  window  to  a  vast  array  of  geospatial  data  sets.  For  example,  the 49  INSPIRE Geoportal [(http://inspire-geoportal.ec.europa.eu/)](http://inspire-geoportal.ec.europa.eu/) provides access to 10,000s of geospatial metadata 50  data records from across Europe. The INSPIRE Geoportal uses a visualization interface comprising of both a 51  map window and folksonomy tag cloud to assist users in navigating this rich geospatial data resource. 

52 

53  There has been an explosion of data available and as our planet continues to experience significant  rapid 54  urbanization, there is an increasing need to access and visualize data which represents the dimensions of space 55  and place (see Straumann et al. (2014) for a distinction of the terms space and place). On this note, the rise of 56  *smart cities* has led to the instrumentation of cities with more real-time data and historical data being captured 57  and visualized (Cheshire & Batty 2012). Large-scale projects such as the Urban Big Data Centre (UBDC) 58  [(http://ubdc.ac.uk/)](http://ubdc.ac.uk/) and the Australian Urban Research Infrastructure Network (AURIN) [(http://aurin.org.au/)](http://aurin.org.au/) 

66  66 60 

61 

62 

63 

64 

66 

` `1  are endeavoring to develop “big data” visualization tools and techniques to support the realization of smart  2  cities. The challenge is to be able to provide not only such data visualization interfaces to researchers, but also  3  tools to support policy and decision makers, city planners and communities to be able to visually explore and  4  analyze this data to make better decisions in collectively planning our cities. 

` `5 

` `6  For example, addressing a rapidly urbanizing Australia, AURIN has developed a geoportal where over 1,800  7  datasets can be accessed and visualized. AURIN has deployed a federated data architecture which is metadata  8  driven (Sinnott et al. 2015). There are over 6 billion data elements that urban researchers, government policy  9  and decision makers can access via the AURIN portal (Pettit et al. 2014). This rich tapestry of “big data” across 10  the domains of health, housing, transport, demographics, economics and other essential areas  provides coverage 11  across the major cities of Australia. Both *point in time* and *longitudinal data* are accessible via the AURIN 12  portal. Other examples show how geospatial big data is being used in practice in connection with multi-user 13  (collaborative) visualization environments in the context of geoportals for visualizing big data through work 14  being undertaken in Europe through INSPIRE and in Australia through AURIN.  

15 

16  Another “cutting edge” research area is the visualization of networks, such as transportation networks (Cheshire 17  & Batty 2012). In the London Oyster card case mentioned before in Section 2, there is a need to be able to 18  visualize  individual  and  aggregated  travel  journeys  to  provide  further  insights  in  the  travel  behaviors  of 19  commuters as they move through the city, and this, in turn, provides important information to transport planners 20  in optimizing timetabling and responding to events. Figure 1 illustrates a snapshot of a time sequence animation 21  of a typical weekday travel based on the Oyster card data. 

22 

23  ![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.012.png)24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42 

43  Figure 1 Oyster Card data for London Tube and train stations, animated for a day using 10 minute intervals 44  (Created by Oliver O’Brien (http://oobrien.com/2013/03/londons-tidal-oyster-card-flow/) (via Batty 2012). 

45 

46  In addition, the real-time visualization of crowd-sourced big data from platforms such as Twitter can assist with, 47  for example, disaster management responses. The visualization of real-time data streaming from these sources 48  can provide emergency response team critical information on how to respond to events such as flood, fires and 49  other  natural  and  human  induced  disasters.  Visualization  platforms  such  as  Ushahidi 50  [(http://www.ushahidi.com/)](http://www.ushahidi.com/)  and  Cognicity  [(http://cognicity.info/cognicity/)](http://cognicity.info/cognicity/)  and  the  Peta  Jakarta  project 51  [(http://petajakarta.org/banjir/en/)](http://petajakarta.org/banjir/en/) are such crowd-sourced platforms. In the latter, citizens can tweet the reporting 52  of floods particularly in the Monsoon season in the city of Jakarta, Indonesia.  

53 

54 

55 

56 

57 

58 

` `1  **6.  Data Mining and Knowledge Discovery** 

` `2 

` `3 

` `4  General big data analysis and traditional spatial data analysis and geo-processing methods and theories can all  5  contribute  to  the  development  of  geospatial  big  data  analysis  and  processing.  Statistical  analysis,  geo-  6  computing, simulation and data mining methods and techniques can be used alone or together with other types  7  of big data for discovering knowledge from geospatial big data. This section examines traditional knowledge  8  discovery methods and resurgence of fractal analysis in dealing with geospatial big data. 

` `9 

10  ***6.1  Data mining and knowledge discovery*** 

11 

12  Knowledge discovery (KD) is concerned with mining and extracting meaningful patterns and relationships from 13  large datasets that are valid, novel, useful and understandable (Miller & Hanz 2009). The field emerged in 14  response to the need for methods applicable to data that violate the assumptions of traditional statistics. KD 15  methods typically emphasise generalisation ability and predictive performance, which is particularly pertinent 16  with spatio-temporal data because spatio-temporal datasets can provide rich information about how a process 17  evolves over time. KD from spatio-temporal data enables us to create models that are able to predict future 18  states of a process, so called the holy grail of science (Cressie & Wikle 2011). KD encompasses a range of 19  spatio-temporal data mining (STDM) tools and methodologies for carrying out a set of tasks. 

20 

21 

22  Pseaerrhchapisn gthfoe r massosotciatioconcenspitun alalydataset simple whKD ere antechevneiqnt uX e teisnd*A*s*s* *s*to*ocia*lead*tion*to *r*an*ule* ev*min*ent *in*Y,*g* w(hAeRre MX ), iswthhiche a nitecenvodlvenest  23  and Y is the consequent (Agrawal et al. 1993). In the context of spatio-temporal data, ARM entails searching for 

24  the occurrence of an event Y in the spatio-temporal neighbourhood of another event X (Mennis & Liu 2005). 25  Shekhar et al. (2011) describe some of the types of patterns that may be present in spatio-temporal data. 

26 

27  More traditional data analysis methods also come under the remit of KD. *Regression* is viewed as a data mining 28  technique, but in truth its use in the spatial sciences and time series analysis predates the field of data mining. 29  Regression  models  for  spatio-temporal  data  emerged  from  the  cross-pollination  of  ideas  from  time  series 30 

31  analysis, econometrics and the spatial sciences. For example, the space-time autoregressive integrated moving 32  average (STARIMA) model (Pfeifer & Deutsch 1980, Cheng et al. 2014), spatial panel data models (Elhorst 33  2003), Bayesian hierarchical models (Cressie & Wikle 2011) and space-time geostatistics (Heuvelink & Griffith 34  2010), amongst others. An important form of regression is classification, in which the outputs are class labels. 35  As  larger  and  more  granular  datasets  have  become  available  in  recent  years,  the  limitations  of  traditional 36  statistical approaches in capturing nonlinearity and heterogeneity have been exposed. Therefore, some scholars 37  have looked to the machine learning (ML) community for alternatives, first to artificial neural networks (ANNs) 38  in the 1990s and more recently to kernel methods. Perhaps the most well-known and broadly successful machine 39  learning (ML) method for classification and regression is the support vector machine (SVM), which uses kernels 40  to carry out nonlinear regression or classification (Kanevski et al. 2009, Haworth et al. 2014). Recently, the 41  Random Forest (RF) which is a method that combines multiple decision trees through bootstrapping has also 42  gained popularity for classification (Cutler et al. 2007).  

43 

44  Other KD tasks include *anomaly detection*, also known as outlier detection, and *clustering*. Anomaly detection 45  involves  the  identification  of  events  or  patterns  in  data  different  from  what  one  would  expect.  Anomaly 46  detection is inherently challenging as it requires a clear definition of what is considered to be normal and 47  abnormal. In spatio-temporal processes, these definitions may evolve and change over time (Chandola et al. 48  2009).  Accounting  for  these  changes  in  different  spatio-temporal  processes  is  a  key  research  challenge. 49  *Clustering* is a form of unsupervised learning, which involves uncovering hidden structure in a dataset about 50  which we know little. Clustering has wide applications in the spatial sciences, for example in geodemographic 51  classification (Vickers & Rees 2007) and hotspot detection (Nakaya & Yano 2010). Although spatial clustering 52  methods  are  well  developed,  spatio-temporal  clustering  (STC)  is  still  an  emerging  research  frontier.  STC 53  methods that are gaining popularity include ST-DBSCAN (Birant & Kut 2007) and  space-time scan statistics 54  (STSS) (Kulldorff et al. 2005, Cheng & Adepeju 2014).  

55 

56  The impact of ML methods on KD and STDM has been significant. ML methods are generally effective in 57  tackling nonlinearity in spatial data, and can be modified to deal with the multi-scale issue and heterogeneity 58  (Foresti et al. 2011). However, in many cases (especially kernel methods) their initial computation is expensive 59  11 60 

` `1  if the number of data samples is large. Furthermore, if the statistical properties of a space-time series evolve  2  over time, models have to be retrained to reflect this. In the big data age where the ability to apply methods to  3  real time data streams is paramount, new ways of training traditional algorithms are needed. In ML, online  4  learning is used  (Castro-Neto et al. 2009) and these types of approaches need to be integrated  with  more  5  traditional spatio-temporal analysis techniques. Parallel and grid computation can also be used to improve the  6  performance  of  KD  methods  (Harris  et  al.  2010).  However,  some  issues  cannot  be  solved  with  only  7  improvements in computational efficiency. For example, the principal problem in STC is to model how clusters  8  emerge, change, move and dissipate/disappear in time. This can be achieved retrospectively but is very difficult  9  to quantify in time critical applications. At what point does a cluster of crimes become a hotspot? Current 10  methodologies within KD and STDM are generally designed to analyse historical datasets but cannot adequately 11  deal with evolving properties of space-time data.  

12 

13  ***6.2  Fractals emerged from big data*** 

14 

15 

16  Big data show incredible fractal structure, in which there are far more small things than large ones (Jiang 2015a, 17  Jiang & Miao 2015). There are several reasons for the emerging fractals. First, big data are usually emerged 18  from the bottom up or are contributed by diverse individuals, e.g., location based social media data, so they are 19  very diverse and heterogeneous. Second, big data are defined at very high spatial and temporal scales, which 20  enable us to observe fractal structure and nonlinear dynamics more easily. Third, big data due to the size are 21  more likely to capture a true picture of reality, and reality is no doubt fractal (Bak 1996, Mandelbrot & Hudson 22  2004). In these aspects, the emerging fractals differ from fractals seen on small data such as fractal cities (Batty 23  & Longley 1994). We argue that fractal geometry (Mandelbrot 1982), or complexity science in general, should 24  be adopted for big data analytics and visualization, and for developing new insights into big data. 

25 

26  Let us examine a working example to see how fractals are generated from tweets location data of the entire 27  world (Jiang 2015a, Jiang & Miao 2015). The data were sliced at different intervals, and in an accumulative 28  manner, which means that locations at t1 are included in locations at t2. For each sliced location dataset, we 29  built a triangulated irregular network (TIN), and merged those small TIN edges (smaller than the mean of all the 30  TIN edges) as individual patches, which are referred as natural cities. Eventually, thousands of the natural cities 31  are emerged from the tweets locations. Figure 2 shows two natural cities near Chicago and New York at the four 32  time instants, and they are put in comparison with the generative fractal – Koch snowflake. The two natural 33  cities look very much like the snowflake, both sharing (1) the scaling pattern of far more small things than large 34  ones, and (2) irregular shapes or boundaries. On the other hand, the natural cities look a bit different from the 35  snowflake; the natural cities are developed from some irregular patches, which become further fragmented, 36  whereas the snowflake and its growth come from the regular triangle with a strict scaling ratio 1/3. In other 37  words, the former is called statistical fractal, being statistically self-similar, while the latter called strict fractal, 38  being strictly self-similar. 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

` `1  ![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.013.png) 2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  Figure 2 Fractals emerged from big data look very much like the generative fractal – Koch snowflake (Jiang 22  2015a) 

23 

24  What the example illustrated are not only fractals or natural cities generated from big data, but also a new, 25  relaxed definition of fractals. A set or pattern is fractal if there are far more small things than large ones in it, or 26  the scaling pattern of far more small things than large ones recurs multiple times (Jiang 2015b, Jiang & Yin 27  2014). The new definition is in fact developed from head/tail breaks (Jiang 2013) as a classification scheme for 28  data with a heavy tailed distribution. The complexity of fractals can be measured by the head/tail breaks induced 29 

30  incadpextured–  hbty- ifnrdacextal :  tdhie menhigsiohern  (thMae nhdtelb-inrdoext 1, 9th82e ),mthoure s hcot-imndpexlexbeinthe g faracn altertal. nCationvve enintiodenxalltoy,f raccotal mpdleimxiteyn swioasn. 31  In comparison to conventional definitions of fractal, the new definition is much more intuitive and easier to 

32  understand, so that anyone can rely on it to see fractals. For example, society is fractal because there are far 33  more poor people than rich people, or far more ordinary people than extraordinary people (Zipf 1949). It should 34  be noted that the head/tail breaks method is not only for data classification, with which both the number of 35  classes and class intervals are automatically determined, but also an efficient and effective visualization tool 36 

37  (Jiang 2015b). Figure 3 presents an example of visualization using the head/tail breaks; only part of the whole is 38  shown to the right panel, but the part is self-similar to the whole, and thus the part reflects the whole. 

39 

40  ![](Aspose.Words.4ad7933d-7723-4cca-bf4b-b73c648d8e49.014.png)41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58 

66  66 60 

61 

62 

63 

64 

66 

` `1  Figure 3 Head/tail breaks as an effective visualization tool (Note: 16,000 natural cities (to the left ) in UK 

` `2  generated from a half million of points of interest look like a hairball, but their top 4 hierarchical levels clearly  3  show a scaling structure to the right.) 

` `4 

` `5  Fractal geometry represents a new way of thinking for geospatial analysis, and this is particularly truly for big  6  data analytics. Despite that Euclidean geometry has thousands of years of history, and serves as the foundation  7  of geospatial technologies, the essence of geography (both physical and human) is fractal. We therefore must  8  adopt fractal methods for developing new insights into big data. This fractal thinking is in line with Paretian  9  thinking (Jiang 2015b), which is in contrast to conventional Gaussian thinking. Statistically speaking, there are 10  far more small things than large ones, rather than more or less similar things. Given the scaling pattern or the 11  heavy tailed distribution, the head/tail breaks provide an effective means to derive an inherent hierarchy of 12  complex systems. This is the first step towards an understanding of geographic phenomena, i.e., to recognize 13  fractal  structure  of  geographic  systems.  The  next  step  is  to  further  develop,  through  simulations,  an 14  understanding of processes as to why the fractal structures exist. In this regard, a set of complexity modeling 15  tools have been developed such as cellular automata, agent-based modeling, and the sand pile model. 

16 

17 

18  **7.  Challenges, Open Issues and Future Directions** 

19 

20 

21  This section presents the challenges and open issues based on the reviews in Sections 2-6 and outlines some 22  research directions in the three focus areas of ISPRS TC II [(http://www2.isprs.org/commissions/comm2.html)](http://www2.isprs.org/commissions/comm2.html). 23 

24  ***7.1  Efficient representation and modeling for geospatial big data*** 

25 

26  The main problem that spatial algorithms face in the context of real-time big data handling is that they cannot 27 

28  wait until all the data are known, as it is the case in two of the major classes of spatial algorithms: the divide- 29  and-conquer or line/plane-sweep algorithms. Even though incremental algorithms are well-suited for handling 30  changing data sets, they are not well-suited for handling streams that cannot fit in the main memory of the 31  computer. Therefore, a new class of spatial algorithms has started to be designed and developed: spatial data 32  streaming algorithms (see e.g., (Hoffmann et al. 2007) and (Sharifzadeh et al. 2009)). However, any real-time 33  streaming algorithms must read an input stream, process it, and write the output stream in real-time. Among all 34  the  tasks  that  have  to  be  performed  on  computers  or  digital  circuits,  the  tasks  that  are  the  slowest,  i.e., 35  communication over a network, must be minimized in order to satisfy the real-time constraint of big data 36  processing. Another implication of the constraint of real-time processing of big spatial data is to minimize the 37  amount of disk input/output, which is the second slowest task after communications over a network. For this 38  purpose, one needs to focus on the most compact data structures that will store spatial data using the smallest 39  amount of memory. Finally, as spatial data tend to be more complex than non-spatial data, the total CPU 40  running time is not negligible with respect to the disk input/output time. Thus, the parallelization of the spatial 41  algorithms will bring a non-negligible speed-up to the spatial algorithm. This means the spatial algorithms must 42  be distributed, parallelized and use the most compact spatial data structures so that the exchange of information 43  over a network and between the disk and the main memory are minimized and the CPU running time is also 44  minimized. This imposes a distributed parallel architecture where streams of data are processed in real time on 45  each unit controlling a sensor in order to transmit over the network only summary statistics and required results 46  for the other nodes in the computing environment. Such summary statistics that are relevant for sensors are 47  intervals, in particular the new measurements that change the intervals of measurements or the required results. 48  Interval analysis is a well-known method for computing bounds of a function, being given bounds on the 49  variables of that function, (see (Moore et al. 2009, p223) for an introduction to interval analysis). The basic 50  mathematical object in interval analysis is the interval instead of the variable. The operators need to be redefined 51  to operate on intervals instead of real variables. This leads to interval arithmetic. In the same way, the most 52  usual mathematical functions are redefined by an interval equivalent. Interval analysis allows one to certify 53  computations  on  intervals  by  providing  bounds  on  the  results.  The  uncertainty  of  each  measure  can  be 54  represented using an interval defined by either a lower bound and a higher bound or a midpoint value and a 55  radius.  

56 

57  Furthermore, interval analysis is in conjunction with functional analysis, the most-well-suited framework to 58  model the uncertainty of geospatial big data. First, as observed by Juditsky et al. (1995), functional analysis 

66  66 60 

61 

62 

63 

64 

66 

` `1  methods like wavelets do not suffer from the curse of dimensionality that affects machine-learning as well as  2  parametric  statistics  (including  multivariate  statistics).  Second,  interval  analysis  allows  one  to  model  the  3  uncertainty of the input variables (like sensor observations) and the corresponding uncertainty of the functions  4  that are evaluated on these variables. The key challenge and open issue is to bridge the gap between the machine  5  learning and functional analysis communities by convincing the communities working on and with geospatial  6  big data to use functional analysis and interval analysis with a functional programming language. Unfortunately,  7  despite  functional  programming  has  made  its  way  into  other  programming  language  paradigms,  functional  8  analysis combined with interval analysis has not been so successful in making its way in applications of big  9  spatial data.  

10 

11  Four initial observations can be seen as the base for exploring further research directions. Firstly, functional 12  analysis  methods  like  wavelets,  homotopy  continuation  and  interval  analysis  are  much  better  suited  than 13  parametric statistical methods to cope with the curse of dimensionality inherent in big spatial data that includes 14  many dimensions (each functionally independent physical value measured corresponds to one dimension and 15  each one of the coordinate systems component x, y and z corresponds also to one dimension). Secondly, pure 16  functional programming is very well suited for handling functions because pure functional programming does 17  not  have  (unlike  impure  functional  programming  and  other  programming  paradigms)  side  effects  in  pure 18  functions,  and  functions  are  one  of  the  two  most  fundamental  concepts  in  any  functional  programming 19  language: functions and data types (see the seminal paper of Hughes (1989)). Thirdly, the main challenge in 20  spatial data handling, which is to certify the topological relationships and the uncertainty of any spatial data 21  modelling  or  decision-making  by  determining  the  uncertainties  of  all  input  variables,  can  be  solved  using 22  interval  analysis.  Finally,  the  lazy  functional  programming  paradigm,  which  postpones  the  evaluation  of 23  expressions until these expressions need to be computed in order to compute the final result (Hughes 1989), is 24  very well suited for fractals due to its ability to represent fractal recurrences and to compute only the fractal 25  components needed to compute the final result. 

26 

27  One  future  research  focus  in  his  area  is  to  produce  a  locally  distributed  stream  sensing,  processing  and 28  telecommunicating paradigm implemented using: 

29  -  new functional specification methods derived from ontologies, ontology mappings and their gluing into 30  ontology categories and their charts; 

31 

32  -  fmuatnctiohemnaal tical analmyosdisellimnegthiondhs ig(hderecodmimpeonssitioionnal s mofeasstrureae mspsacwes ithfroimnterinvteral vael nclovalsuuedres hoomf owtoavpies eletsin, 33 

34  -  lopuwre erf udnimctioennsal ionpal romgreaamsumrie nsgplanaces);gua ges (see (haskell.org committee, 2015, Hughes 1989)) with:  

35 

36  -  visualization libraries (like Haskell and its OpenGL bindings and many specialized visualization 37  libraries), 

38  -  parallelization and concurrency libraries (like the distributed MapReduce framework Holumbus or 39  OpenCL and OpenMP wrappers), 

40  -  cloud computing libraries; 

41  -  functional programming inspired parallelisation and concurrency techniques. 

42 

43  The architecture of any system based on this paradigm can be considered a fractal. Every sensor controlling unit 44  is responsible for collecting the big data streams, computing the statistics or any other desired result, generating 45  the  triggers  that  will  automatically  update  any  computed  result,  visualization  or  decision  making  and 46  transmitting it to a lower-resolution data collecting node. Each local data-collecting node is responsible for 47  storing the streams of results and providing the desired visualization and assembling the partial decision taking 48  elements into a summarized decision taking from the neighbouring sensor controlling units. Each regional data 49  collecting node is responsible for storing the streams of results and providing the desired visualization and 50  assembling the partial decision taking elements into a summarized decision taking from the neighbouring local 51  data collecting nodes. 

52 

53  The other future research focus on new processing algorithms to handle large volumes of data through use of 54  functional programming languages is to design new streaming algorithms that: 

55  -  use the provably most compact geometric topology data structure that encodes all the ramifications (i.e. 56  the singular points of the skeletons of the objects), 

57  -  use CPU and GPU parallelization to harness the computing facilities wherever these lie (locally), 

58  -  use interval analysis to:  

66  66 60 

61 

62 

63 

64 

66 

` `1  -  represent uncertainty of streams of spatial data (Dilo et al. 2007), and  

` `2  -  to automatically generate the triggers that will react automatically once an input value in a stream  3  will make a change in any computed result, visualization or decision making; 

` `4  -  use wavelet decompositions for handling signals acquired by sensors, 

` `5  -  use fractals for handling spatial data  that has components whose dimension is not an integer (the  6  Hausdorff-Besicowitz dimension of a fractal is not an integer, and thus, fractals can be used to model  7  phenomena that exhibit self-similarity), 

` `8  -  use interval valued homotopies to model or reconstruct functions in higher dimensional measure spaces  9  from  measurements  and  reconstructions  of  functions  from  lower  dimensional  measure  spaces, 10  reconstructing therefore higher dimensional measure spaces one dimension at a time, 

11  -  use categories to represent the functional dependencies between data variables and any computed result 12  or visualization or decision taking (like data types and functions in the Hask category), and a category 13  based Domain Specific Language computing library (like docon, see Mechveliani (2001)). 

14 

15 

16  ***7.2  Analyzing, mining and visualizing geospatial big data for decision-support*** 17 

18  Spatial statistical methods are in principle able to also include non-spatial big data. A typical example is the use 19  of co-variables in a spatial interpolation procedure (Van de Kassteele & Stein 2006), or in a spatial point process 20  modelling analysis. Such analyses commonly rely on assumptions, such as normality, independence, absence of 21  noise in the explanatory data. With the advancements of big data, however, on the one hand the quality of the 22  big data can be doubted, whereas on the other hand the speed of calculations is seriously affected. Hence, the 23  procedures are at the moment not ready for the purpose, and serious pre-processing has to be done, for which the 24  tools are not readily available. Some of the methods have been developed in the past, such as possibilities to use 25  explanatory variables of a changing quality, but these are often rather complicated to use, and require substantial 26  work to make them available for the purpose. As an example, one may think of Bayesian procedures, where 27  prior distributions can be included into a likelihood function. These methods have shown an enormous power in 28  spatial studies but their application on an automatic basis for big data may not be so easy and transparent to 29  develop, implement and apply. 

30 

31  The main challenges in KD and STDM can be summarised around the first four *Vs* described in Section 1.  

32 

33  *Volumes* of spatio-temporal data are ever increasing and some argue that traditional  transactional database 34  structures are becoming outmoded. Although this point is open to debate, research is taking place into how to 35  best use new data storage and query architectures to deal with spatio-temporal data. For example, recent studies 36  have  used  MapReduce  programming  models  to  parallelise  data  processing  (Tan  et  al.  2012).  Despite  this, 37  traditional SQL based spatial databases, such as PostGIS and Oracle Spatial, remain dominant in academia. If 38  this trend continues, there is a risk that academic research will become disconnected from industry.  

39 

40  Despite  significant  progress,  challenges  remain  in  developing  predictive  algorithms  that  can  deal  with  the 41  *velocity* of data arriving in real time. Geography has been a big data discipline since long before the term arose, 42  dealing  with  large  and  complex  problems  like  weather  and  climate  modelling.  Hence,  geography  has 43  traditionally been forward thinking regarding the development of algorithms for dealing with large and complex 44  datasets in a timely fashion. The discourse on parallel computing in geography began in the 1990s, leading to 45  the emergence of the subfield of geocomputation (Cheng et al. 2012). However, parallelism is still far from the 46  common  practice  despite  enabling  hardware  being  available  in  desktop  computers.  Most  recent  work  has 47  focussed on parallelising all or part of existing algorithms to improve computational performance. For example, 48  Guan and Clarke (2010) developed a parallel raster processing library for use in cellular automata, and Guan et 49 

50  al.  (2011)  developed  algorithms  to  parallelize  elements  of  Kriging  interpolation.  Libraries  for  parallel 51  geocomputation are also now beginning to emerge in open source software environments such as R (Harris et al. 52  2010). Building on this, a greater focus needs to be placed on developing algorithms that are parallel in nature 53  and can harness all types of parallelism. This is what Turton and Openshaw (1998, p. 1842) termed “Thinking in 54  Parallel” in 1998, but is yet to be fully adopted in the research community.  

55 

56  *Variety* presents itself as an opportunity to the research community. We now have potentially many data sources 57  of different types that can be used to analyse (and re-analyse) a diverse range of spatio-temporal processes. 58  Traditional data providers such as governments, national mapping agencies and transport authorities are now 

66  66 60 

61 

62 

63 

64 

66 

` `1  complemented  by  new  data  sources  described  in  Section  2.  However,  an  important  and  often  overlooked  2  problems of KD in the spatial sciences is data *veracity*. Big data provides unprecedented volumes of data about  3  a broad range of human activities and physical processes. However, they are often collected on a fairly ad-hoc  4  basis when compared with traditional data sources, and usually must be repurposed to fulfil research objectives.  5  Initiatives such as OpenStreetMap have proven that crowd sourced data can compete with official sources in this  6  regard, but issues of sample representativeness and data collection design (or lack thereof) are still a concern. A  7  good example is Twitter, which has gained popularity in research communities recently, but under represents  8  certain groups, including the elderly and some ethnic minorities, and has an uneven spatio-temporal distribution.  9  Such datasets clearly have potential value alongside traditional demographic data such as censuses and cross- 10  sectional  surveys  (Longley  et  al.  2015).  However,  there  is  a  trade-off  between  the  spatial  and  temporal 11  granularity offered by these data and the certainty associated with any conclusions drawn from them. Personal 12  mobility data is another example; smartphone apps collect and store vast quantities of such data which have 13  considerable  research  potential,  but  this  cannot  be  fully  realised  without  proper  validation  and  a  clear 14  understanding of potential biases in the user group. This is coupled with difficulties in inferring activities from 15  raw, unlabelled mobility data (Bolbol et al. 2011).  

16 

17  Sampling is not a legitimate concept in the big data era, as argued by Jiang and Miao (2015). Big data tends to 18  take all or a large amount, and this data characteristic make big data different from small data which are often 19  sampled. Surely, social  media data are  oriented  towards  younger  generations or those  who have access to 20  Internet and social media, and not everyone has Internet access, in particular in developing countries. However, 21  the large data volumes enable big data to capture the true picture of all social media users, despite the fact that 22  not all people are involved in social media. All social media users can be a good proxy for studying real human 23  activities on the earth surface. In this regard, social media data are like maps, which do not represent everything 24  on the earth surface, but can be a good proxy of earth surface. Big data calls for new ways of thinking beyond 25  conventional thinking (Jiang 2015b, Mayer-Schönberger & Cukier 2013). For example, we tend to examine data 26  quality as we did in the small data era. Massive data should tolerate messiness. For navigation purposes, we 27  must make OSM data as precise and accurate as possible, in other words, the more precise and the more 28  accurate, the better. However, to examine whether there are far more small street blocks than large ones, we do 29  not need very good quality data, but need massive data with messiness instead. 

30 

31  Nonetheless,  visualizing  big  data  has  some  additional  challenges;  for  example,  visual  analytics  solutions 32  themselves may not scale: we need to consider how to deal with *information overload* on the viewer if we show 33  too many things at the same time (Choo & Park 2013, Ruff 2002). Applications in a large-scale geoportal like 34  AURIN  have  revealed  there  are  significant  challenges  in  being  able  to  visualize  large  multi-dimensional 35  geospatial datasets via a browser. For this, thousands of years of cartographic expertise offers sound advice. We 36  should  carefully  generalize,  e.g.,  emphasize  the  important  while  removing  the  unimportant,  group  the 37  information both thematically and perceptually, and pay attention to visual hierarchy when we design displays. 38 

39  W(Mae cEshoaculdhr enre met eal.mb2er0 0li8n, kSchingnthüre erget ooal.d ca20rto14g,rapSlohcuic mdesiget aln.  p2r0in0cip8). les Furtoth ermmodoerren,  riesearnteracchtioerns  indesigthe ncaprartoadgrigapmhys  40  and geovisualization domain have taken a strong interest in cognitive and usability issues and much progress has 

41 

42  been  made  to  understand  how  human  capacity  can  enhance  or  limit  our  experiences  with  visual  displays 43  (Çöltekin et al. 2010, Knapp 1995, Montello 2002, Roth 2013, Slocum et al. 2001). 

44 

45  Whilst the *smart city* and *big data* are hot topics of today (Batty 2012), there is also the challenge of how to 46  visualize the error and uncertainty inherent with big data sets such as crowd-sourced datasets and smart card 47  data (Cheshire & Batty 2012). In the realm of geospatial big data, there is also a need to effectively visualize 48  other  types  of  massive  data  sets,  such  as  LIDAR  data  or  very  large  collections  of  remote-sensing  data. 49  Visualization platforms such as PointCloudViz [(http://www.pointcloudviz.com/)](http://www.pointcloudviz.com/), or Online LIDAR point cloud 50  viewer (http://lidarview.com/) specialize on Lidar data visualization. Similarly, there are efforts in organizing 51  remotely sensed imagery (Ma et al. 2014, Marshall & Boshuizen 2013). 

52 

53  Spatial statistics has always had good opportunities for visualizing spatial and spatio-temporal data, including 54  the  uncertainties  (see  Rulinda  et  al.  (2013)  for  an  excellent  example  in  the  space-time  domain).  Their 55  opportunities are still present also for geospatial big data. In the past, such methods have also been applied to 56  non-spatial data, and there is apparently not a real problem to extend them towards big data. A critical issue in 57  all of spatial (or spatio-temporal) data is their relying on coordinates. Hence, also for non-spatial data there must 58  be an opportunity  to assign  coordinates, or equivalent, to the  non-spatial data. Successes in the past have 

66  66 60 

61 

62 

63 

64 

66 

` `1  typically considered class memberships to serve that purpose, bringing the non-spatial data towards the feature  2  space (Dilo et al. 2007). 

` `3 

` `4  ***7.3  Quality assessment of geospatial big data from new sources*** 

` `5 

` `6  Dealing with veracity in a scalable and timely manner has been identified as a substantial challenge (Saha &  7  Srivastava 2014). Behind this challenge is the lack of thorough knowledge of the data semantics when data that  8 

` `9  is commonly called “dirty” (e.g., Chiang & Miller 2008) is collected and combined in an ad-hoc manner. 10  Accordingly, big data analytics has given up the closed-world assumption in favour of learning incrementally 11  under an open-world assumption. Thus, big data quality assessment has been characterized by the three stages of 12  discovering rules of data semantics, checking for inconsistencies based on currently known rules, and repairing 13  data near real-time (Saha & Srivastava 2014). 

14 

15  Geospatial big data collection, especially sensors’ data, is being captured in an automatic and unprecedented 16  way, which poses new opportunities and challenges. This in principle causes problems for a statistical analysis, 17  where statistical considerations such as optimal design or model based sampling are critical to make valid 

statements. Big spatial data may be either largely irrelevant, as they are collected automatically and hence much 1189   uninteresting repetition may occur, whereas big spatial data may be of a highly varying quality. They may be 

20  precise, but too precise to be of use, they may be too abundant as the same object is over sampled, they may be 21  of a very poor quality as data may be at the nominal scale, where ratio-scale data might have been collected 22  elsewhere, or they may be inadequate for the specific purpose. At the moment, no adequate procedures seem to 23  be implemented to be able to harmonize the quality of spatial data for specific purposes. This will require a 24  significant effort in order to develop feasible solutions. 

25 

26  On the positive side, the large number of data sources allows us to acquire a “complete picture” of a spatial 27  situation, also including its dynamics. This is even more so, when different sensor data are integrated and fused, 28  allowing to also eliciting more than one aspect or feature of an object (Ebert et al. 2009). Due to the potential 29  high redundancy, it is possible to identify errors or blunders in the data and achieve higher accuracies even 30  though an individual sensor has a limited measuring quality.  

31 

32  As often no explicit semantics (or manual annotation of semantics) is given, automatic processes are required to 33  reveal it. This refers both to the object and its features, but also to the temporal characteristics. 

34 

35  Crowd-sourced data sets are often related to very detailed, i.e., large-scale phenomena. However, the level of 36  granularity (in space, time and semantics) is not  always  known. Hence, it is a challenge to determine this 37  granularity level from the sensor data. One option is to include explicit metadata about the sensor in terms of a 38  self-description. Another option is to infer the scale and granularity level by automatically relating it to other 39  sources of known granularity. This involves matching techniques at both the geometric and semantic levels.  

40 

41  In traditional sensors the quality (e.g., geometric accuracy) of the acquired data is given. This is not necessarily 42  the case  with  new data sources such as VGI data.  As this data  source  is often  user centred, questions of 43  reputation and trust have to be included to evaluate the quality. 

44 

45 

46  **8.  Conclusions** 

47 

48 

49  This study reviews a variety of geospatial theory and methods used for traditional data but that can be extended 50  to handle geospatial big data. While there is no standard definition of big data, it can be considered as structured 51  and  unstructured  datasets  with  massive  data  volumes  that  cannot  be  easily  captured,  stored,  manipulated, 52  analysed, managed and presented by traditional hardware, software and  database technologies. Given these 53  unique characteristics, traditional data handling approaches and methods are inadequate and the following areas 54  were identified as in need for further development and research in the discipline: 

55 

56  §  The development of new spatial indexing and algorithms to handle real-time, streaming data and to support 57  topology for real-time analytics. 

58 

66  66 60 

61 

62 

63 

64 

66 

` `1  §  The development of conceptual and methodological approaches to move big data from descriptive and  2  correlation research and applications to ones that explore casual and explanatory relationships.  

` `3  §  The development of efficient methods to display data integrated in the three dimensions of geographic and  4  one dimension of continuous time. There is a strong need in understanding human capacity to deal with  5  visual information and identifying which visualization type is a good fit for the task at hand, and the target  6  user  group.  Furthermore,  interdisciplinary  studies  and  communication  is  important.  The  advances  in  7  scientific visualization and information visualization are both beneficial to geographic visualization; but  8  geographic visualization has also a lot to offer to other domains. Novel visualization paradigms, especially  9  developed for big data tend to be information-rich (thus complex); therefore, we find that highlighting and 10  summarizing approaches should be further investigated. Additionally, and in relation to managing complex 11  visualization displays, technology research in terms of level of detail management remains important. 

12  §  The development of novel approaches for error propagation so as to effectively assess data quality requires. 13  The challenge is not only the handling the many different types of data for real-time analytics, but rather the 14  ad-hoc combination of data streams in real-time, which may include the capture of the “whole” picture (or 15  “complete population”) instead of sampling a small portion of the whole population. In this case quick 16  assessments are preferable that may come out of varying the input data and simulating variability. 

17 

18  The paper also highlighted other general conceptual and practical issues. The relation between spatial statistics 19  and semantics and ontologies has been identified in the past, but requires further elaboration. Ontologies have 20  been identified, for example, for dunes and beaches in studies around 2005, whereas more slum ontologies have 21  been developed. The role of spatial statistics was related to the scale, the environment and the characterization 22  of specific variables. In particular, aspects of scale are important. 

23 

24  Privacy and security are equally important and key concerns especially in geospatial big data handling, which 25  may lead us into a “naked future” if not properly addressed. They are an essential part of geospatial big data 26  management, but are not covered here given the focus on this paper on data handling methods. 

27 

28  Big  data  presents  both  challenges  and  opportunities.  This  paper  outlines  some  of  these  challenges  from  a 29  technical and conceptual perspective, and also provides priority areas that need to be addressed in the future. 30  Once big data research evolves and matures, the opportunities from leveraging big data for overall societal 31  management and decision-making become enormous. 

32 

33 

34  **Acknowledgment** 

35 

36  The partial support of this study was funded by NSERC Discover Grants awarded separately to the first and 37  second authors. The authors thank Anthony Lee (Spatial Analysis and Modeling Laboratory,  Simon Fraser 38  University, Department of Geography) for assistance in compiling the reference database and formatting the 39  citations.  

40 

41 

42  **Authors Contribution Statement** 

43 

44 

45  Regardless of the listed order of the authors, all authors contributed equally by participating in discussions, 46  writing sections, revising corresponding sections and providing revision comments on the entire paper.    

47 

48 

49  **References** 

50 

51 

52  Acar, U. A., & Chen, Y. (2013). Streaming Big Data with Self-Adjusting Computation. *Proceedings of the 2013* 53  *Workshop  on  Data  Driven  Functional  Programming  -  DDFP  ’13*,  15-18.  doi: 54  10.1145/2429376.2429382 

55  Ackermann,  F.  (1994).  Digital  Elevation  Models  -  Techniques  and  Applications,  Quality  Standards, 56  Development. *IAPRS, 30/4*(Commission IV), 421-432.  

57 

58 

66  66 60 

61 

62 

63 

64 

66 

` `1  Agrawal, R., Imieliński, T., & Swami, A. (1993). Mining Association Rules between Sets of Items in Large  2  Databases. *Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data,*  3  *22*(2), 207-216. doi: 10.1145/170035.170072 

` `4  Anthony, D., Henderson, T., & Kotz, D. (2007). Privacy in Location-Aware Computing Environments. *IEEE*  5  *Pervasive Computing, 6*(4), 64-72.  

` `6  Bak, P. (1996). *How Nature Works: The Science of Self-Organised Critically* (1 ed.): Copernicus. 

` `7  Batty, M. (2012). Smart Cities, Big Data. *Environmental and Planning B: Planning and Design, 39*(2), 191-193.  8  doi: 10.1068/b3902ed 

` `9  Batty, M., & Longley, P. A. (1994). *Fractal Cities: A Geometry of Form and Function*. London: Academic 10  Press. 

11  Bektas,  K.,  &  Çöltekin,  A.  (2012).  Area  of  Interest  Based  Interaction  and  Geovisualization  with  WebGL. 12  *Proceedings of The Graphical Web Conference 2012*.  

13  Bernasocchi,  M.,  Çöltekin,  A.,  &  Gruber,  S.  (2012).  An  Open  Source  Geovisual  Analytics  Toolbox  for 14  Multivariate  Spatio-Temporal  Data  for  Environmental  Change  Modeling.  *ISPRS  Annals  of  the* 15  *Photogrammetry, Remote Sensing and Spatial Information Sciences, 1-2*(2), 123-128.  

16  Bolbol, A., Cheng, T., & Haworth, J. (2011). Using a Moving Window SVMs Classification to Infer Travel 17  Mode from GPS Data. *Proceedings of the 11th International Conference on GeoComputation*, 262- 18  270.  

19  Carbin,  M.,  Misailović,  S.,  &  Rinard,  M.  C.  (2013).  Verifying  Quantitative  Reliability  for  Programs  that 20  Execute  on  Unreliable  Hardware.  *ACM  SIGPLAN  NOTICES  -  OOPSLA  '13,  48*(10),  33-52.  doi: 21  10.1145/2544173.2509546 

22  Castro-Neto, M., Jeong, Y.-S., Jeong, M.-K., & Han, L. D. (2009). Online-SVR for short-term traffic flow 23  prediction under typical and atypical traffic conditions. *Expert Systems with Applications 36*, 6164– 24  6173. doi: 10.1016/j.eswa.2008.07.069 

25  Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly Detection: A Survey. *ACM Computing Surveys,* 26  *41*(3), 1-58. doi: 10.1145/1541880.1541882 

27  Chen, M., Mao, S., & Liu, Y. (2014). Big Data: A Survey. *Mobile Networks and Applications, 19*(2), 171-209. 28  doi: 10.1007/s11036-013-0489-0 

29  Cheng, T & Adepeju, M. (2014). Modifiable Temporal Unit Problem (MTUP) and its Effect on Space-Time 30  Cluster Detection. PLOS ONE 9(6), e100465. 

31  Cheng, T., Haworth, J., Anbaroglu, B., Tanaksaranond, G., & Wang, J. (2014). Spatiotemporal Data Mining. In 32  M. M. Fischer & P. Nijkamp (Eds.), *Handbook of Regional Science* (pp. 1173-1193): Springer Berlin 33  Heidelberg. 

34  Cheng,  T.,  Haworth,  J.,  &  Manley,  E.  (2012).  Advances  in  Geocomputation  (1996-2011).  *Computers,* 35  *Environment and Urban Systems, 36*(6), 481-487. doi: 10.1016/j.compenvurbsys.2012.10.002 

36  Cheshire, J., & Batty, M. (2012). Visualisation Tools for Understanding Big Data. *Environment and Planning* 37  *B: Planning and Design, 39*(3), 413-415.  

38  Cheng, T., Wang, J., Haworth, J., Heydecker, B., Chow, A. (2014). A Dynamic Spatial Weight Matrix and 39  Localized  Space–Time  Autoregressive  Integrated  Moving  Average  for  Network  Modeling, 40 

Geographical Analysis. Geographical Analysis, 46: 75–97 DOI: 10.1111/gean.12026. 

41 

Chiang, F., & Miller, R. J. (2008). Discovering Data Quality Rules. *Proceedings of the VLDB Endowment, 1*(1), 42 

43  1166-1177. doi: 10.14778/1453856.1453980 

44  Choo, J., & Park, H. (2013). Customizing Computational Methods for Visual Analytics with Big Data. *IEEE* 45  *Computer Graphics and Applications, 33*(4), 22-28.  

46  Cockburn,  A.,  Karlson,  A.,  &  Bederson,  B.  B.  (2008).  A  Review  of  Overview+Detail,  Zooming,  and 47  Focus+Context Interfaces. *ACM Computing Surveys, 41*(1), 1-31. doi: 10.1145/1456650.1456652 

48  Çöltekin, A. (2009). Space-Variant Image Coding for Stereoscopic Media. *Picture Coding Symposium, 2009* 49  *(PCS 2009)*, 1-4. doi: 10.1109/PCS.2009.5167396 

50  Çöltekin,  A.,  Fabrikant,  S.  I.,  &  Lacayo,  M.  (2010).  Exploring  the  Efficiency  of  Users'  Visual  Analytics 51  Strategies  Based  on  Sequence  Analysis  of  Eye  Movement  Recordings.  *International  Journal  of* 52  *Geographical Information Science, 24*(10), 1559-1575. doi: 10.1080/13658816.2010.511718 

53  Çöltekin,  A.,  &  Reichenbacher,  T.  (2011).  High  Quality  Geographic  Services  and  Bandwidth  Limitations. 54  *Future Internet, 3*(4), 379-396. doi: 10.3390/fi3040379 

55  Cook, K., Grinstein, G., Whiting, M., Cooper, M., Havig, P., Liggett, K., . . . Paul, C. L. (2012). VAST 56  Challenge 2012: Visual Analytics for Big Data. *Proceedings of the 2012 IEEE Conference on Visual* 57  *Analytics Science and Technology (VAST)*, 251-255. doi: 10.1109/VAST.2012.6400529 

58 

66  66 60 

61 

62 

63 

64 

66 

` `1  Crampton, J. W., Graham, M., Poorthuis, A., Shelton, T., Stephens, M., Wilson, M. W., & Zook, M. (2013).  2  Beyond the Geotag: Situating ‘Big Data’and Leveraging the Potential of the Geoweb. *Cartography and*  3  *Geographic Information Science, 40*(2), 130-139. doi: 10.1080/15230406.2013.777137 

` `4  Cressie, N., & Wikle, C. K. (2011). *Statistics for Spatio-Temporal Data*: John Wiley & Sons. 

` `5  Cutler, D. R., Edwards, T. C. Jr., Beard, K. H., Cutler, A., Hess, K. T., Gibson, J., & Lawler, J.  J., 2007.  6  Random Forests for Classification in Ecology. Ecology, 88(11), 2783-2792.  

` `7  Dasgupta, A. (2013). Big Data: The future is in analytics. *Geospatial World*. 

` `8  Deogawanka,  S.  (2014).  Empowering  GIS  with  Big  Data.    Retrieved  May  4,  2015,  from  9  http://www.gislounge.com/empowering-gis-big-data/ 

10  Dilo, A., By, R. A. d., & Stein, A. (2007). A system of types and operators for handling vague spatial objects. 11  *International  Journal  of  Geographical  Information  Science,  21*(4),  397-426.  doi: 12  10.1080/13658810601037096 

13  Dittrich, A., & Lucas, C. (2013). A Step Towards Real-Time Detection and Localization of Disaster Events 14  Based on Tweets. *Proceedings of the 10th International Conference on Information Systems for Crisis* 15  *Response and Management*, 868-872.  

16  Duckham, M. (2013). *Decentralized Spatial Computing*: Springer Heidelberg. 

17  Duckham, M., & Kulik, L. (2006). Location Privacy and Location-Aware Computing. *Dynamic & mobile GIS:* 18  *Investigating change in space and time, 3*, 34-51.  

19  Ebert, A., Kerle, N., & Stein, A. (2009). Urban social vulnerability assessment with physical proxies and spatial 20  metrics derived from air- and spaceborne imagery and GIS data. *Natural Hazards, 48*(2), 275-294. doi: 21  10.1007/s11069-008-9264-0 

22  Elhorst, J. P. (2003). Specification and Estimation of Spatial Panel Data Models. *International Regional Science* 23  *Review, 26*(3), 244-268. doi: 10.1177/0160017603253791 

24  Evans, M. R., Oliver, D., Zhou, X., & Shekhar, S. (2014). Spatial Big Data: Case Studies on Volume, Velocity, 25  and Variety. In H. A. Karimi (Ed.), *Big Data: Techniques and Technologies in Geoinformatics* (pp. 26  149-176): CRC Press. 

27  Fitzner, D., Sester, M., Haberlandt, U., & Rabiei, E. (2013). Rainfall Estimation with a Geosensor Network of 28  Cars - Theoretical Considerations and First Results. *Photogrammetrie-Fernerkundung-Geoinformation,* 29  *2013*(2), 93-103. doi: 10.1127/1432-8364/2013/0161 

30  Foresti, L., Tuia, D., Kanevski, M., & Pozdnoukhov, A. (2011). Learning wind fields with multiple kernels 31  *Stochastic Environmental Research and Risk Assessment, 25*(1), 51-66. doi: 10.1007/s00477-010-0405- 32  0 

33  Frank, A. U. (2001). Tiers of Ontology and Consistency Constraints in Geographical Information Systems. 34  *International Journal of Geographical Information Science, 15*(7), 667-678.  

35  Frank,  A.  U.  (2007).  Data  Quality  Ontology:  An  Ontology  for  Imperfect  Knowledge.  In  S.  Winter,  M. 36  Duckham, L. Kulik & B. Kuipers (Eds.), *Spatial Information Theory* (pp. 406-420): Springer Berlin 37  Heidelberg. 

38  Frankel,  F.,  &  Reid,  R.  (2008).  Big  Data:  Distilling  Meaning  from  Data.  *Nature,  455*(7209),  30-30.  doi: 39  10.1038/455030a 

40 

Frommberger, L., Schmid, F., & Cai, C. (2013). Micro-Mapping with Smartphones for Monitoring Agricultural 41 

Development. *Proceedings of the 3rd ACM Symposium on Computing for Development*, 46.  

42 

43  Fuchs, G., Andrienko, N., Andrienko, G., Bothe, S., & Stange, H. (2013). Tracing the German Centennial Flood 44  in  the  Stream  of  Tweets:  First  Lessons  Learned.  *Proceedings  of  the  Second  ACM  SIGSPATIAL* 45  *International Workshop on Crowdsourced and Volunteered Geographic Information*, 31-38.  

46  Gomes, L. (2014, October 20, 2014). Machine-Learning Maestro Michael Jordan on the Delusions of Big Data 47  and  Other  Huge  Engineering  Efforts.  Retrieved  May  4,  2015,  from 48  http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-

49  the-delusions-of-big-data-and-other-huge-engineering-efforts 

50  Goodchild, M. F. (2007). Citizens as Sensors: The World of Volunteered Geography. *GeoJournal, 69*, 211-221. 51  doi: 10.1007/s10708-007-9111-y 

52  Goodchild, M. F. (2013). The Quality of Big (Geo)data. *Dialogues in Human Geography, 3*(3), 280-284. doi: 53  10.1177/2043820613513392 

54  Goodchild, M. F., & Glennon, A. (2010). Crowdsourcing Geographic Information for Disaster Response: A 55  Research Frontier. *International Journal of Digital Earth, 3*(3), 231-241.  

56  Guan, Q., & Clarke, K. C. (2010). A General-Purpose Parallel Raster Processing Programming Library Test 57  Application Using a Geographic Cellular  Automata  Model.  *International Journal of Geographical* 58  *Information Science, 24*(5), 695-722. doi: 10.1080/13658810902984228 

66  66 60 

61 

62 

63 

64 

66 

` `1  Guan, Q., Kyriakidis, P. C., & Goodchild, M. F. (2011). A Parallel Computing Approach to Fast Geostatistical  2  Areal Interpolation. *International Journal of Geographical Information Science, 25*(8), 1241-1267. doi:  3  10.1080/13658816.2011.563744 

` `4  Haklay,  M.  (2010).  How  Good  is  Volunteered  Geographical  Information?  A  Comparative  Study  of  5  OpenStreetMap and Ordnance Survey Datasets. *Environment and Planning B: Planning and Design,*  6  *37*(4), 682-703. doi: 10.1068/b35097 

` `7  Harris,  R.,  Singleton,  A.,  Grose,  D.,  Brunsdon,  C.,  &  Longley,  P.  (2010).  Grid‐enabling  Geographically  8  Weighted Regression: A Case Study of Participation in Higher Education in England. *Transactions in*  9  *GIS, 14*(1), 43-61. doi: 10.1111/j.1467-9671.2009.01181.x 

10  Haworth, J., Shawe-Taylor, J., Cheng, T., Wang, J. (2014) Local online kernel ridge regression for forecasting 11  of  urban  travel  times.  Transportation  Research  Part  C:  Emerging  Technologies  46,  151–178. 12  doi:10.1016/j.trc.2014.05.015. 

13  Hegarty, M. (2011). The Cognitive Science of Visual-Spatial Displays: Implications for Design.  *Topics  in* 14  *Cognitive Science, 3*(3), 446-474. doi: 10.1111/j.1756-8765.2011.01150 

15  Hegarty, M., Smallman, H. S., & Stull, A. T. (2012). Choosing and Using Geospatial Displays: Effects of 16  Design on Performance and Metacognition. *Journal of Experimental Psychology: Applied, 18*(1), 1-17. 17  doi: 10.1037/a0026625 

18  Heuvelink, G., & Griffith, D. A. (2010). Space–Time Geostatistics for Geography: A Case Study of Radiation 19  Monitoring Across Parts of Germany. *Geographical Analysis, 42*(2), 161-179. doi: 10.1111/j.1538- 20  4632.2010.00788.x 

21  Hoffer, D. (2014). What Does Big Data Look Like? Visualisation is Key for Humans.   Retrieved May 4, 2015, 22  from http://www.wired.com/2014/01/big-data-look-like-visualization-key-humans/ 

23  Hoffman, J. (2012). Q&A: The Data Visualizer. *Nature, 486*(7401), 33-33. doi: 10.1038/486033a 

24  Hoffmann, M., Raman, R., & Muthukrishnan, S. (2007). Streaming algorithms for data in motion. Lecture Notes 25  in Computer Science (including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in 26  Bioinformatics), 4614 LNCS, 294–304. 

27  Huang, W., Li, S., Liu, X., & Ban, Y. (2015). Predicting human mobility with activity changes. *International* 28  *Journal of Geographical Information Science*. doi: 10.1080/13658816.2015.1033421  

29  Hughes, J. (1989). Why Functional Programming Matters. Computer Journal, Computer Journal, Computer J, 30  Comput J, Comput. J, 32(2), 98–107. doi:10.1093/comjnl/32.2.98 

31  Jiang, B. (2013). Head/tail Breaks: A New Classification Scheme for Data with a Heavy-tailed Distribution. *The* 32  *Professional Geographer, 65*(3), 482-494. doi: 10.1080/00330124.2012.700499  

33  Jiang, B. (2015a). Head/tail Breaks for Visualization of City Structure and Dynamics. *Cities, 43*, 69-77.  

34  Jiang,  B.  (2015b).  Geospatial  Analysis  Requires  a  Different  Way  of  Thinking:  The  Problem  of  Spatial 35  Heterogeneity. *GeoJournal, 80*(1), 1-13. doi: 10.1007/s10708-014-9537-y 

36  Jiang, B., & Miao, Y. (2015). The Evolution of Natural Cities from the Perspective of Location-Based Social 37  Media. *The Professional Geographer, 67*(2), 295-306. doi: 10.1080/00330124.2014.968886 

38  Jiang, B., & Yin, J. (2014). Ht-Index for Quantifying the Fractal or Scaling Structure of Geographic Features. 39 

*Annals  of  the  Association  of  American  Geographers,  104*(3),  530-540.  doi: 40 

10\.1080/00045608.2013.834239 

41 

42  Juditsky, A., Hjalmarsson, H., Benveniste, A., Delyon, B., Ljung, L., Sjöberg, J., & Zhang, Q. (1995). Nonlinear 43  Black-Box Models in System Identification: Mathematical Foundations.  *Automatica, 31*(12), 1725- 44  1750. doi: 10.1016/0005-1098(95)00119-1 

45  Kandwal, R., Augustijn, E.-W., Stein, A., Miscione, G., Garg, P. K., & Garg, R. D. (2010). Geospatial analysis 46  of HIV-Related social stigma: A study of tested females across Indian mandals. *International Journal* 47  *of Health Geographics, 9*(18).  

48  Kanevski, M., Pozdnoukhov, A., & Timonin, V. (2009). *Machine Learning for Spatial Environmental Data:* 49  *Theory, Applications, and Software*: EFPL Press. 

50  Kealy,  A.,  Retscher,  G.,  Toth,  C.,  &  Brzezinska,  D.  (2014).  Collaborative  Positioning:  Concepts  and 51  Approaches for More Robust Positioning. *Proceedings of the XXV FIG Congress 2014: Engaging the* 52  *Challenges Enhancing the Relevance*, 15.  

53  Keim, D., Qu, H., & Ma, K.-L. (2013). Big-Data Visualization. *IEEE Computer Graphics and Applications,* 54  *33*(4), 20-21.  

55  Kitchin,  R.  (2014).  Big  Data,  new  epistemologies  and  paradigm  shifts.  Big  Data  &  Society,  1(1),  1-12. 56  doi:10.1177/2053951714528481. 

57  Kitchin, R. (2013). Big Data and Human Geography Opportunities, Challenges and Risks. *Dialogues in Human* 58  *Geography, 3*(3), 262-267. doi: 10.1177/2043820613513388 

66  66 60 

61 

62 

63 

64 

66 

` `1  Knapp, L. (1995). A Task Analysis Approach to the Visualization of Geographic Data. In T. L. Nyerges, D. M.  2  Mark, R. Laurini & M. J. Egenhofer (Eds.), *Cognitive Aspects of Human-Computer Interaction for*  3  *Geographic Information Systems* (pp. 355-371): Springer Netherlands. 

` `4  Kohli, D., Sliuzas, R., Kerle, N., & Stein, A. (2012). An ontology of slums for  image-based classification.  5  *Computers,  Environment  and  Urban  Systems,  36*(2),  154-163.  doi:  6  10.1016/j.compenvurbsys.2011.11.001 

` `7  Kulldorff, M., Heffernan, R., Hartman, J., Assunção, R., & Mostashari, F. (2005). A Space-Time Permutation  8  Scan  Statistic  for  Disease  Outbreak  Detection.  *PLoS  Medicine,  2*(3),  e59.  doi:  9  10.1371/journal.pmed.0020059 

10  Laney, D. (2001). 3D Data Management: Controlling Data Volume, Velocity, and Variety. *Application Delivery* 11  *Strategies*.  http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management- 12  Controlling-Data-Volume-Velocity-and-Variety.pdf 

13  Lee,  K.,  Ganti,  R.  K.,  Srivatsa,  M.,  &  Liu,  L.  (2014).  Efficient  Spatial  Query  Processing  for  Big  Data. 14  *Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic* 15  *Information Systems - SIGSPATIAL ’14*, 469-472. doi: 10.1145/2666310.2666481 

16  Li, Y., Yan, D, & Liu, S. (2011). Dimensionality Reduction Algorithm Based on Density Portrayal. Computer 17  Engineering, 37(21), 138–140. 

18  Liu, Z., Jiang, B., & Heer, J. (2013). ImMens: Real-Time Visual Querying of Big Data. *Computer Graphics* 19  *Forum (Proceedings of EuroVis '13), 32*(3pt4), 421-430.  

20  Longley,  P.  A.,  Adnan,  M.,  &  Lansley,  G.  (2015).  The  Geotemporal  Demographics  of  Twitter  Usage. 21  *Environment and Planning A, 47*(2), 465-484. doi: 10.1068/a130122p 

22  Lu, Y., Zhang, M., Witherspoon, S., Yesha, Y., Yesha, Y., & Rishe, N. (2013). sksOpen: Efficient Indexing, 23  Querying,  and  Visualization  of  Geo-spatial  Big  Data.  *Proceedings  of  the  2013  12th International* 24  *Conference  on  Machine  Learning  and  Applications  (ICMLA),  2*,  495-500.  doi: 25  10.1109/ICMLA.2013.196 

26  Ma, Y., Wu, H., Wang, L., Huang, B., Ranjan, R., Zomaya, A., & Jie, W. (2014). Remote Sensing Big Data 27  Computing:  Challenges  and  Opportunities.  *Future  Generation  Computer  Systems*.  doi: 28  10.1016/j.future.2014.10.029 

29  MacEachren, A. M., Crawford, S., Akella, M., & Lengerich, G. (2008). Design and Implementation of a Model, 30  Web-Based,  GIS-Enabled  Cancer  Atlas.  *The  Cartographic  Journal,  45*(4),  246-260.  doi: 31  10.1179/174327708X347755 

32  Maitrey,  S.,  &  Jha,  C.  K.  (2015).  *Handling  Big  Data  Efficiently  by  Using  Map  Reduce  Technique.* Paper 33  presented  at  the  The  2015  IEEE  International  Conference  on  Computational  Intelligence  & 34  Communication Technology (CICT), Ghaziabad, IN. 

35  Mandelbrot, B. B. (1982). *The Fractal Geometry of Nature*. New York: W. H. Freeman. 

36  Mandelbrot, B. B., & Hudson, R. L. (2004). *The (Mis)behavior of Markets: A Fractal View of Risk, Ruin and* 37  *Reward*. New York: Basic Books. 

38  Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big Data: The 39  Next Frontier for Innovation, Competition, and Productivity. 

40 

Marshall,  W.,  &  Boshuizen,  C.  (2013).  Planet  Labs’  Remote  Sensing  Satellite  System.  *Small  Satellite* 41 

42  *Conference*.  

43  Mayer-Schönberger, V., & Cukier, K. (2013). *Big Data: A Revolution That Will Transform How We Live, Work,* 44  *and Think*. New York: Eamon Dolan/Houghton Mifflin Harcourt. 

45  McDougall, K. (2011). Using Volunteered Information to Map the Queensland Floods.  *Proceedings of the* 

*Surveying & Spatial Sciences Biennial Conference 2011*, 13-24.  

4467   Mechveliani, S. D. (2001). Computer algebra with Haskell: applying functional-categorial-‘lazy’ programming, 48  Proceedings  of  International  Workshop  CAAP-2001  (Dubna,  Russia),  2001,  203–211;  http://ca-

49  d.jinr.ru/confs/CAAP/Final/proceedings/proceed.ps 

50  Mennis, J., & Liu, J. W. (2005). Mining Association Rules in Spatio‐Temporal Data: An Analysis of Urban 51  Socioeconomic  and  Land  Cover  Change.  *Transactions  in  GIS,  9*(1),  5-17.  doi:  10.1111/j.1467- 52  9671.2005.00202.x 

53  Milewski, B. (2009). Lock Options. *DR DOBBS JOURNAL, 34*(1), 28-31.  

54  Miller, H. J., & Goodchild, M. (2014). Data-Driven Geography. *GeoJournal (Online First)*, 1-13.  

55  Miller, H. J., & Hanz, J. (2009). Geographic Data Mining and Knowledge Discovery: An Overview *Geographic* 56  *Data Mining and Knowledge Discovery* (Second Edition ed., pp. 1-26): CRC Press. 

57  Mintchev, S. (2014). User–Defined Rules Made Simple with Functional Programming. In W. Abramowicz & A. 58  Kokkinaki (Eds.), *Business Information Systems* (pp. 229-240): Springer International Publishing. 

66  66 60 

61 

62 

63 

64 

66 

` `1  Mohammed, E. A., Far, B. H., & Naugler, C. (2014). Applications of the MapReduce Programming Framework  2  to Clinical Big Data Analysis: Current Landscape and Future Trends. *BioData Mining, 7*(1), 22. doi:  3  10.1186/1756-0381-7-22 

` `4  Mondzech, J., & Sester, M. (2011). Quality Analysis of OpenStreetMap Data Based on Application Needs.  5  *Cartographica: The International Journal for Geographic Information and Geovisualization, 46*(2),  6  115-125.  

` `7  Montello, D. R. (2002). Cognitive Map-Design Research in the Twentieth Century: Theoretical and Empirical  8  Approaches.  *Cartography  and  Geographic  Information  Science,  29*(3),  283-304.  doi:  9  10.1559/152304002782008503 

10  Moore,  R.E.,  Kearfott,  R.B.,  &  Cloud,  M.J.  (2009).  Introduction  to  interval  analysis  (pp.  223):  SIAM,  11  Philadelphia. 

12  Morais, C. D. (2012). Where is the Phrase “80% of Data is Geographic” From? .   Retrieved May 2, 2015, from 13  http://www.gislounge.com/80-percent-data-is-geographic/  

14  Musiige,  D.,  Anton,  F.,  &  Mioc,  D.  (2013).  *RF  Subsystem  Power  Consumption  and  Induced  Radiation* 15  *Emulation.* (Ph.D.), Technical University of Denmark, Kongens Lyngby.    

16  Musiige, D., Anton, F., Yatskevich, V., Vincent, L., Mioc, D., & Pierre, N. (2011). RF Power Consumption 17  Emulation  Optimized  with  Interval  Valued  Homotopies.  *Proceedings  of  the  World  Academy  of* 18  *Science, Engineering and Technology, 81*, 147-153.  

19  Nakaya, T., & Yano, K. (2010). Visualising Crime Clusters in a Space-time Cube: An Exploratory Data-analysis 20  Approach Using Space-time Kernel Density Estimation and Scan Statistics. *Transactions in GIS, 14*(3), 21  223-239. doi: 10.1111/j.1467-9671.2010.01194.x 

22  Neis, P., Zielstra, D., & Zipf, A. (2012). The Street Network Evolution of Crowdsourced Maps: OpenStreetMap 23  in Germany 2007-2011. *Future Internet 2012, 4*(1), 1-21. doi: 10.3390/fi4010001 

24  Pettit, C., Stimson, R., Nino-Ruiz, M., Moradini, L., Widjaja, I., Delaney, P., . . . Kvan, T. (2014). Supporting 25  Urban Informatics through a Big Data Analytics Online Workbench. *NSF Workshop on Big Data and* 26  *Urban Informatics*.  

27  Poser, K., Kreibich, H., & Dransch, D. (2009). Assessing Volunteered Geographic Information for Rapid Flood 28  Damage  Estimation  *Proceedings  of  the  12th  AGILE  International  Conference  on  Geographic* 29  *Information Science: Advances in GIScience*.  

30  Richter, K.-F., & Winter, S. (2011). Citizens as Database: Conscious Ubiquity in Data Collection. In D. Pfoser, 31  Y. Tao, K. Mouratidis, M. A. Nascimento, M. Mokbel, S. Shekhar & Y. Huang (Eds.), *Advances in* 32  *Spatial and Temporal Databases* (pp. 445-448): Springer Berlin Heidelberg. 

33  Riebeek,  H.  (2015).  Big  Data  Helps  Scientists  Dig  Deeper.    Retrieved  May  23,  2015,  from 34  http://earthobservatory.nasa.gov/Features/LandsatBigData/ 

35  Roth, R. E. (2013). An Empirically-Derived Taxonomy of Interaction Primitives for Interactive Cartography and 36  Geovisualization. *IEEE Transactions on Visualization and Computer Graphics, 19*(12), 2356-2365. 37  doi: 10.1109/TVCG.2013.130 

38  Ruff, J. (2002). Information Overload: Causes, Symptoms and Solutions (pp. 1-13): Harvard Graduate School of 39  Education’s Learning Innovations Laboratory (LILA)  

40 

Rulinda, C. M., Stein, A., & Turdukulov, U. D. (2013). Visualizing and quantifying the movement of vegetative 41 

drought  using  remote-sensing  data  and  GIS.  *International  Journal  of  Geographical  Information* 42 

43  *Science, 27*(8), 1481-1496. doi: 10.1080/13658816.2012.723712 

44  Saha, B., & Srivastava, D. (2014). Data Quality: The Other Face of Big Data. *Proceedings of the 2014 IEEE* 45  *30th International Conference on Data Engineering (ICDE)*, 1294-1297.  

46  Schnürer, R., Sieber, R., & Çöltekin, A. (2014). The Next Generation of Atlas User Interfaces: A User Study 47  with  “Digital  Natives".  In  J.  Brus,  A.  Vondrakova  &  V.  Vozenilek  (Eds.),  *Modern  Trends  in* 48  *Cartography* (pp. 23-36): Springer International Publishing. 

Sester, M., Arsanjani, J. J., Klammer, R., Burghardt, D., & Haunert, J.-H. (2014). Integrating and Generalising 4590   Volunteered Geographic Information  *Abstracting Geographic Information in a Data Rich World  -* 

51  *Methodologies and Applications of Map Generalisation* (pp. 119-155): Springer Heidelberg. 

52  Sharifzadeh, M., & Shahabi, C. (2009). Approximate voronoi cell computation on spatial data streams. Vldb 53  Journal, Vldb J, 18(1), 57–75.  http://doi.org/10.1007/s00778-007-0081-y 

54  Shekhar, S., Evans, M. R., Gunturi, V., Yang, K., & Cugler, D. C. (2014). Benchmarking Spatial Big Data. In T. 55  Rabl,  M.  Poess,  C.  Baru  &  H.-A.  Jacobsen  (Eds.),  *Specifying  Big  Data  Benchmarks*  (pp. 81-93): 56  Springer Berlin Heidelberg. 

57 

58 

66  66 60 

61 

62 

63 

64 

66 

` `1  Shekhar, S., Evans, M. R., Kang, J. M., & Mohan, P. (2011). Identifying Patterns in Spatial Information: A  2  Survey of Methods. *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1*(3),  3  193-214. doi: 10.1002/widm.25 

` `4  Shekhar, S., Gunturi, V., Evans, M. R., & Yang, K. (2012). Spatial Big-Data Challenges Intersecting Mobility  5  and Cloud Computing. *Proceedings of the 11th ACM International Workshop on Data Engineering for*  6  *Wireless and Mobile Access - MobiDE ’12*. doi: 10.1145/2258056.2258058 

` `7  Shneiderman,  B.  (2014).  The  Big  Picture  for  Big  Data:  Visualization.  *Science,  343*(6172),  730-730.  doi:  8  10.1126/science.343.6172.730-a 

` `9  Sinnott, R. O., Bayliss, C., Bromage, A., Galang, G., Grazioli, G., Greenwood, P., . . . Widjaja, I. (2015). The 10  Australia Urban Research Gateway. *Concurrency and Computation: Practice and Experience, 27*(2), 11  358-375. doi: 10.1002/cpe.3282 

12  Slocum, T. A., Blok, C., Jiang, B., Koussoulakou, A., Montello, D. R., Fuhrmann, S., & Hedley, N. R. (2001). 13  Cognitive and Usability Issues in Geovisualization. *Cartography and Geographic Information Science,* 14  *28*(1), 61-75. doi: 10.1559/152304001782173998 

15  Slocum,  T.  A.,  McMaster,  R.  B.,  Kessler,  F.  C.,  &  Hugh,  H.  H.  (2008).  *Thematic  Cartography  and* 16  *Geovisualization, 3rd Edition*: Prentice Hall. 

17  Steed, C. A., Ricciuto, D. M., Shipman, G., Smith, B., Thornton, P. E., Wang, D., . . . Williams, D. N. (2013). 18  Big  Data  Visual  Analytics  for  Exploratory  Earth  System  Simulation  Analysis.  *Computers  &* 19  *Geosciences, 61*, 71-82. doi: 10.1016/j.cageo.2013.07.025 

20  Stein, A., Groenigen, J. W. v., Jeger, M. J., & Hoosbeek, M. R. (1998). Space-time statistics for environmental 21  and agricultural related phenomena. *Environmental and Ecological Statistics, 5*(2), 155-172.  

22  Straumann,  R.  K.,  Çöltekin,  A.,  &  Andrienko,  G.  (2014).  Towards  (Re)Constructing  Narratives  from 23  Georeferenced Photographs through Visual Analytics. *The Cartographic Journal, 51*(2), 152-165. doi: 24  10.1179/1743277414Y.0000000079 

25  Suthaharan, S. (2014). Big Data Classification: Problems and Challenges in Network Intrusion Prediction with 26  Machine Learning. *Performance Evaluation Review, 41*(4), 70-73. doi: 10.1145/2627534.2627557 

27  Tan, H., Luo, W., & Ni, L. M. (2012). CloST: A Hadoop-Based Storage System for Big Spatio-Temportal Data 28  Analytics. *Proceedings of the 21st ACM International Conference on Information and Knowledge* 29  *Management*, 2139-2143. doi: 10.1145/2396761.2398589 

30  Tran, N., Skhiri, S., Lesuisse, A., & Zimányi, E. (2012). *AROM: Processing big data with Data Flow Graphs* 31  *and functional programming.* Paper presented at the The 4th IEEE International Conference on Cloud 32  Computing Technology and Science (CloudCom), Taipei. 

33  Turton, I., & Openshaw, S. (1998). High-Performance Computing and Geography: Developments, Issues, and 34  Case Studies. *Environment and Planning A, 30*(10), 1839-1856. doi: 10.1068/a301839 

35  Ujang, U., Anton, F., Azri, S., Rahman, A. A., & Mioc, D. (2014). 3D Hilbert Space Filling Curves in 3D City 36  Modeling for Faster Spatial Queries. *International Journal of 3-D Information Modeling (IJ3DIM),* 37  *3*(2). doi: 10.4018/ij3dim.2014040101 

38  Umamaheshwaran,  R.,  Bijker,  W.,  &  Stein,  A.  (2007).  Image  Mining  for  Modeling  of  Forest  Fires  From 39  Meteosat  Images.  *IEEE  Transactions  on  Geoscience  and  Remote  Sensing,  45*(1),  246-253.  doi: 40 

10\.1109/TGRS.2006.883460 

41 

Van de Kassteele, J., & Stein, A. (2006). A model for external drift kriging with uncertain covariates applied to 42 

43  air  quality  measurements  and  dispersion  model  output.  *Environmetrics,  17*(4),  309-322.  doi: 44  10.1002/env.771 

45  Van de Vlag, D., & Stein, A. (2006). Modeling Dynamic Beach Objects Using Spatio-Temporal Ontologies. 46  *Journal of Environmental Informatics, 8*(1), 22-33.  

47  Van de Vlag, D., Vasseur, B., Stein, A., & Jeansoulin, R. (2005).  An application of problem and product 48  ontologies for the revision of beach nourishments. *International Journal of Geographical Information* 49  *Science, 19*(10), 1057-1072. doi: 10.1080/13658810500032404 

50  Van Zyl, T., Simonis, I., & McFerren, G. (2009). The Sensor Web: Systems of Sensor Systems. *International* 51  *Journal of Digital Earth, 2*(1), 16-30. doi: 10.1080/17538940802439549 

52  Veregin, H. (2005). Data Quality Parameters. In M. F. G. Paul A. Longley, David J. Maguire, David W. Rhind 53  (Ed.), *New Developments in Geographical Information Systems: Principles, Techniques, Management* 54  *and Applications* (pp. 177-189). Hoboken, NY: Wiley. 

55  Vickers, D., & Rees, P. (2007). Creating the UK National Statistics 2001 Output Area Classification. *Journal of* 56  *the Royal Statistical Society: Series A, 170*(2), 379-403. doi: 10.1111/j.1467-985X.2007.00466.x 

57 

58 

66  66 60 

61 

62 

63 

64 

66 

` `1  Vlahogianni, E. I., Karlaftis, M. G., & Golias, J. C. (2006). Statistical Methods for Detecting Nonlinearity and  2  Non-Stationarity in Univariate Short-Term Time-Series of Traffic Volume. *Transportation Research*  3  *Part C: Emerging Technologies, 14*(5), 351-367. doi: 10.1016/j.trc.2006.09.002 

` `4  Wang, S., Ding, G., & Zhong, M. (2013). On Spatial Data Mining Under Big Data. *Journal of China Academy*  5  *of Electronics and Information Technology, 8*(1), 8-17.  

` `6  Wang, S., & Yuan, H. (2013). *Spatial Data Mining in the Context of Big Data* Paper presented at the The 2013  7  International Conference on Parallel and Distributed Systems (ICPADS), Seoul, KW. 

` `8  Wang, S., & Yuan, H. (2014). Spatial Data Mining: A Perspective of Big Data. *International Journal of Data*  9  *Warehousing and Mining, 10*(4), 50-70. doi: 10.4018/ijdwm.2014100103 

10  Wood, S. A., Guerry, A. D., Silver, J. M., & Lacayo, M. (2013). Using Social Media to Quantify Nature-Based 11  Tourism and Recreation. *Scientific Reports, 3*, 2976. doi: 10.1038/srep02976 

12  Yan-yan,  L.,  De-qin,  Y.,  &  Sheng-lan,  L.  (2011).  Dimensionality  Reduction  Algorithm  Based  on  Density 13  Portrayal. Computer Engineering, 37(21), 138–140 

14  Zhang, L., Stoffel, A., Behrisch, M., Mittelstadt, S., Schreck, T., Pompl, R., . . . Keim, D. (2012). Visual 15  Analytics for the Big Data Era - A Comparative Review of State-of-the-Art Commercial Systems. 16  *Proceedings of the 2012 IEEE Conference on Visual Analytics Science and Technology (VAST)*, 173- 17  182. doi: 10.1109/VAST.2012.6400554 

18  Zhu, XD, Huang, ZQ, Shen, GH, & Yuan M.  (2009). Variable slide window based frequent itemsets mining 19  algorithm on large data streams. Control and Decision, 24(6), 832-836. 

20  Zielstra, D., & Zipf, A. (2010). A Comparative Study of Proprietary Geodata and Volunteered Geographic 21  Information for Germany. *Proceedings of the 13th AGILE International Conference on Geographic* 22  *Information Science 2010*.  

23  Zipf, G. K. (1949). *Human Behavior and the Principle of Least Effort*. Oxford, UK: Addison-Wesley Press. 

24 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

37 

38 

39 

40 

41 

42 

43 

44 

45 

46 

47 

48 

49 

50 

51 

52 

53 

54 

55 

56 

57 

58 

59  26 60 

61 

62 

63 

64 

65 

[View publication stats](https://www.researchgate.net/publication/283341250)

